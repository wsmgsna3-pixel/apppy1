# -*- coding: utf-8 -*-
"""
ä¸»åŠ›ç­–ç•¥ Â· V36.15 ç©¿è¶Šè€… (T+5 å»¶æ—¶ä¹°å…¥å®éªŒ)
------------------------------------------------
å®éªŒç›®çš„: éªŒè¯"ä¸»åŠ›ç­–ç•¥é€‰è‚¡åï¼Œç¬¬5å¤©æ‰æ˜¯ä¸»å‡æµªèµ·ç‚¹"çš„å‡è®¾ã€‚
æ ¸å¿ƒé€»è¾‘:
1. **é€‰è‚¡**ï¼šå®Œå…¨æ²¿ç”¨ V36.14 çš„é€»è¾‘ (MOM>7, RSIæ— ä¸Šé™, è·åˆ©ç›˜>75%)ã€‚
2. **ä¹°å…¥**ï¼š
   - ä¿¡å·æ—¥: T
   - åŸä¹°å…¥æ—¥: T+1
   - **æ–°ä¹°å…¥æ—¥**: **T+5** (ç¬¬5ä¸ªäº¤æ˜“æ—¥å¼€ç›˜ä»·ä¹°å…¥)ã€‚
3. **è§‚å¯Ÿ**ï¼š
   - Return_Delay_1: T+5 ä¹°å…¥ï¼ŒT+6 å–å‡ºã€‚
   - Return_Delay_3: T+5 ä¹°å…¥ï¼ŒT+8 å–å‡ºã€‚
   - Return_Delay_5: T+5 ä¹°å…¥ï¼ŒT+10 å–å‡ºã€‚
------------------------------------------------
"""

import streamlit as st
import pandas as pd
import numpy as np
import tushare as ts
from datetime import datetime, timedelta
import warnings
import time
import concurrent.futures 
import os
import pickle

warnings.filterwarnings("ignore")

# ---------------------------
# å…¨å±€å˜é‡åˆå§‹åŒ–
# ---------------------------
pro = None 
GLOBAL_ADJ_FACTOR = pd.DataFrame() 
GLOBAL_DAILY_RAW = pd.DataFrame() 
GLOBAL_QFQ_BASE_FACTORS = {} 
GLOBAL_STOCK_INDUSTRY = {} 

# ---------------------------
# é¡µé¢è®¾ç½®
# ---------------------------
st.set_page_config(page_title="ä¸»åŠ›ç­–ç•¥ V36.15 ç©¿è¶Šè€…", layout="wide")
st.title("ä¸»åŠ›ç­–ç•¥ V36.15ï¼šç©¿è¶Šè€… (T+5 å»¶æ—¶ä¹°å…¥)")

# ---------------------------
# åŸºç¡€ API å‡½æ•°
# ---------------------------
@st.cache_data(ttl=3600*12) 
def safe_get(func_name, **kwargs):
    global pro
    if pro is None: 
        return pd.DataFrame(columns=['ts_code']) 
   
    func = getattr(pro, func_name) 
    try:
        for _ in range(3):
            try:
                if kwargs.get('is_index'):
                    df = pro.index_daily(**kwargs)
                else:
                    df = func(**kwargs)
                
                if df is not None and not df.empty:
                    return df
                time.sleep(0.5)
            except:
                time.sleep(1)
                continue
        return pd.DataFrame(columns=['ts_code']) 
    except Exception as e:
        return pd.DataFrame(columns=['ts_code'])

def get_trade_days(end_date_str, num_days):
    lookback_days = max(num_days * 3, 365) 
    start_date = (datetime.strptime(end_date_str, "%Y%m%d") - timedelta(days=lookback_days)).strftime("%Y%m%d")
    cal = safe_get('trade_cal', start_date=start_date, end_date=end_date_str)
    
    if cal.empty or 'cal_date' not in cal.columns:
        return []
        
    trade_days_df = cal[cal['is_open'] == 1].sort_values('cal_date', ascending=False)
    trade_days_df = trade_days_df[trade_days_df['cal_date'] <= end_date_str]
    return trade_days_df['cal_date'].head(num_days).tolist()

@st.cache_data(ttl=3600*24)
def fetch_and_cache_daily_data(date):
    adj_df = safe_get('adj_factor', trade_date=date)
    daily_df = safe_get('daily', trade_date=date)
    return {'adj': adj_df, 'daily': daily_df}

@st.cache_data(ttl=3600*24*7) 
def load_industry_mapping():
    global pro
    if pro is None: return {}
    try:
        sw_indices = pro.index_classify(level='L1', src='SW2021')
        if sw_indices.empty: return {}
        index_codes = sw_indices['index_code'].tolist()
        all_members = []
        load_bar = st.progress(0, text="æ­£åœ¨éå†åŠ è½½è¡Œä¸šæ•°æ®...")
        for i, idx_code in enumerate(index_codes):
            df = pro.index_member(index_code=idx_code, is_new='Y')
            if not df.empty: all_members.append(df)
            time.sleep(0.02) 
            load_bar.progress((i + 1) / len(index_codes), text=f"åŠ è½½è¡Œä¸šæ•°æ®: {idx_code}")
        load_bar.empty()
        if not all_members: return {}
        full_df = pd.concat(all_members)
        full_df = full_df.drop_duplicates(subset=['con_code'])
        return dict(zip(full_df['con_code'], full_df['index_code']))
    except Exception as e:
        return {}

# ---------------------------
# æ•°æ®è·å–æ ¸å¿ƒ
# ---------------------------
CACHE_FILE_NAME = "market_data_cache_v36.pkl"

def get_all_historical_data(trade_days_list, use_cache=True):
    global GLOBAL_ADJ_FACTOR, GLOBAL_DAILY_RAW, GLOBAL_QFQ_BASE_FACTORS, GLOBAL_STOCK_INDUSTRY
    if not trade_days_list: return False
    
    with st.spinner("æ­£åœ¨åŒæ­¥å…¨å¸‚åœºè¡Œä¸šæ•°æ®..."):
        GLOBAL_STOCK_INDUSTRY = load_industry_mapping()

    if use_cache and os.path.exists(CACHE_FILE_NAME):
        st.success(f"âš¡ å‘ç°æœ¬åœ°è¡Œæƒ…ç¼“å­˜ ({CACHE_FILE_NAME})ï¼Œæ­£åœ¨æé€ŸåŠ è½½...")
        try:
            with open(CACHE_FILE_NAME, 'rb') as f:
                cached_data = pickle.load(f)
                GLOBAL_ADJ_FACTOR = cached_data['adj']
                GLOBAL_DAILY_RAW = cached_data['daily']
                
            latest_global_date = GLOBAL_ADJ_FACTOR.index.get_level_values('trade_date').max()
            if latest_global_date:
                try:
                    latest_adj_df = GLOBAL_ADJ_FACTOR.loc[(slice(None), latest_global_date), 'adj_factor']
                    GLOBAL_QFQ_BASE_FACTORS = latest_adj_df.droplevel(1).to_dict()
                except: GLOBAL_QFQ_BASE_FACTORS = {}
            
            st.info("âœ… æœ¬åœ°ç¼“å­˜åŠ è½½æˆåŠŸï¼")
            return True
        except Exception as e:
            st.warning(f"ç¼“å­˜æ–‡ä»¶æŸåï¼Œå°†é‡æ–°ä¸‹è½½: {e}")
            os.remove(CACHE_FILE_NAME)

    latest_trade_date = max(trade_days_list) 
    earliest_trade_date = min(trade_days_list)
    
    start_date_dt = datetime.strptime(earliest_trade_date, "%Y%m%d") - timedelta(days=200)
    end_date_dt = datetime.strptime(latest_trade_date, "%Y%m%d") + timedelta(days=30)
    
    start_date = start_date_dt.strftime("%Y%m%d")
    end_date = end_date_dt.strftime("%Y%m%d")
    
    all_trade_dates_df = safe_get('trade_cal', start_date=start_date, end_date=end_date, is_open='1')
    
    if all_trade_dates_df.empty or 'cal_date' not in all_trade_dates_df.columns:
        st.error("âŒ æ— æ³•è·å–äº¤æ˜“æ—¥å†æ•°æ®ã€‚")
        return False
        
    all_dates = all_trade_dates_df['cal_date'].tolist()
    
    st.info(f"ğŸ“¡ [é¦–æ¬¡è¿è¡Œ] æ­£åœ¨ä¸‹è½½æ•°æ®: {start_date} è‡³ {end_date} (ä¸‹è½½åå°†è‡ªåŠ¨ç¼“å­˜)...")

    adj_factor_data_list = [] 
    daily_data_list = []

    def fetch_worker(date):
        return fetch_and_cache_daily_data(date)

    progress_text = "Tushare æ•°æ®ä¸‹è½½ä¸­..."
    my_bar = st.progress(0, text=progress_text)
    total_steps = len(all_dates)
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
        future_to_date = {executor.submit(fetch_worker, date): date for date in all_dates}
        for i, future in enumerate(concurrent.futures.as_completed(future_to_date)):
            try:
                data = future.result()
                if not data['adj'].empty: adj_factor_data_list.append(data['adj'])
                if not data['daily'].empty: daily_data_list.append(data['daily'])
            except Exception as exc: pass
            
            if i % 5 == 0 or i == total_steps - 1:
                my_bar.progress((i + 1) / total_steps, text=f"ä¸‹è½½ä¸­: {i+1}/{total_steps}")

    my_bar.empty()
    
    if not daily_data_list:
        st.error("âŒ æ•°æ®åŒæ­¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ä¼‘æ¯ç‰‡åˆ»å†è¯•ã€‚")
        return False
   
    with st.spinner("æ­£åœ¨æ„å»ºç´¢å¼•å¹¶ä¿å­˜ç¼“å­˜..."):
        adj_factor_data = pd.concat(adj_factor_data_list)
        adj_factor_data['adj_factor'] = pd.to_numeric(adj_factor_data['adj_factor'], errors='coerce').fillna(0)
        GLOBAL_ADJ_FACTOR = adj_factor_data.drop_duplicates(subset=['ts_code', 'trade_date']).set_index(['ts_code', 'trade_date']).sort_index(level=[0, 1]) 
        
        daily_raw_data = pd.concat(daily_data_list)
        GLOBAL_DAILY_RAW = daily_raw_data.drop_duplicates(subset=['ts_code', 'trade_date']).set_index(['ts_code', 'trade_date']).sort_index(level=[0, 1])

        latest_global_date = GLOBAL_ADJ_FACTOR.index.get_level_values('trade_date').max()
        if latest_global_date:
            try:
                latest_adj_df = GLOBAL_ADJ_FACTOR.loc[(slice(None), latest_global_date), 'adj_factor']
                GLOBAL_QFQ_BASE_FACTORS = latest_adj_df.droplevel(1).to_dict()
            except: GLOBAL_QFQ_BASE_FACTORS = {}
        
        try:
            with open(CACHE_FILE_NAME, 'wb') as f:
                pickle.dump({'adj': GLOBAL_ADJ_FACTOR, 'daily': GLOBAL_DAILY_RAW}, f)
            st.success("ğŸ’¾ è¡Œæƒ…æ•°æ®å·²ç¼“å­˜åˆ°ç¡¬ç›˜ï¼Œä¸‹æ¬¡é‡å¯å°†ç§’å¼€ï¼")
        except Exception as e:
            st.warning(f"ç¼“å­˜å†™å…¥å¤±è´¥: {e}")
            
    return True

# ---------------------------
# å¤æƒè®¡ç®—æ ¸å¿ƒé€»è¾‘
# ---------------------------
def get_qfq_data_v4_optimized_final(ts_code, start_date, end_date):
    global GLOBAL_DAILY_RAW, GLOBAL_ADJ_FACTOR, GLOBAL_QFQ_BASE_FACTORS
    if GLOBAL_DAILY_RAW.empty: return pd.DataFrame()
    
    latest_adj_factor = GLOBAL_QFQ_BASE_FACTORS.get(ts_code, np.nan)
    if pd.isna(latest_adj_factor): return pd.DataFrame() 

    try:
        daily_df = GLOBAL_DAILY_RAW.loc[ts_code]
        daily_df = daily_df.loc[(daily_df.index >= start_date) & (daily_df.index <= end_date)]
        adj_series = GLOBAL_ADJ_FACTOR.loc[ts_code]['adj_factor']
        adj_series = adj_series.loc[(adj_series.index >= start_date) & (adj_series.index <= end_date)]
    except KeyError: return pd.DataFrame()
    
    if daily_df.empty or adj_series.empty: return pd.DataFrame()
    
    df = daily_df.merge(adj_series.rename('adj_factor'), left_index=True, right_index=True, how='left')
    df = df.dropna(subset=['adj_factor'])
    
    for col in ['open', 'high', 'low', 'close', 'pre_close']:
        if col in df.columns:
            df[col + '_qfq'] = df[col] * df['adj_factor'] / latest_adj_factor
    
    df = df.reset_index().rename(columns={'trade_date': 'trade_date_str'})
    df = df.sort_values('trade_date_str').set_index('trade_date_str')
    
    for col in ['open', 'high', 'low', 'close']:
        df[col] = df[col + '_qfq']
        
    return df[['open', 'high', 'low', 'close', 'vol']].copy() 

# ---------------------------
# å®æˆ˜ä»¿çœŸä¸æŒ‡æ ‡è®¡ç®—
# ---------------------------
# [MODIFIED] å»¶æ—¶ä¹°å…¥é€»è¾‘
def get_future_prices_delayed(ts_code, selection_date, delay_days=5, days_ahead=[1, 3, 5]):
    # è·å–æ›´é•¿çš„æœªæ¥æ•°æ®ï¼Œä»¥ä¾¿å®šä½åˆ°ç¬¬5å¤©
    d0 = datetime.strptime(selection_date, "%Y%m%d")
    start_future = (d0 + timedelta(days=1)).strftime("%Y%m%d")
    end_future = (d0 + timedelta(days=30)).strftime("%Y%m%d") # å»¶é•¿æ—¶é—´çª—å£
    
    hist = get_qfq_data_v4_optimized_final(ts_code, start_date=start_future, end_date=end_future)
    results = {}
    
    # å¿…é¡»è¦æœ‰è¶³å¤Ÿçš„æ•°æ® (è‡³å°‘èƒ½æ”¯æŒåˆ° delay_days)
    if hist.empty or len(hist) < delay_days: return results
    
    hist['open'] = pd.to_numeric(hist['open'], errors='coerce')
    hist['close'] = pd.to_numeric(hist['close'], errors='coerce')
    
    # è·å–ç¬¬ 5 ä¸ªäº¤æ˜“æ—¥çš„æ•°æ® (ç´¢å¼•æ˜¯ delay_days - 1)
    # ä¾‹å¦‚ delay=5, åˆ™æ˜¯ç¬¬ 1,2,3,4, [5] å¤©
    buy_day_idx = delay_days - 1
    buy_day_data = hist.iloc[buy_day_idx]
    
    # å‡è®¾ä»¥ T+5 çš„å¼€ç›˜ä»·ä¹°å…¥
    target_buy_price = buy_day_data['open']
    
    # è®¡ç®— T+5 ä¹‹åçš„æ”¶ç›Š (Delay+1, Delay+3, Delay+5)
    # å³ T+6, T+8, T+10
    for n in days_ahead:
        col = f'Return_Delay_{n}'
        sell_idx = buy_day_idx + n 
        
        if len(hist) > sell_idx:
            sell_price = hist.iloc[sell_idx]['close']
            results[col] = (sell_price - target_buy_price) / target_buy_price * 100
        else:
            results[col] = np.nan
            
    return results

def calculate_rsi(series, period=12):
    delta = series.diff()
    gain = (delta.where(delta > 0, 0)).ewm(alpha=1/period, adjust=False).mean()
    loss = (-delta.where(delta < 0, 0)).ewm(alpha=1/period, adjust=False).mean()
    rs = gain / (loss + 1e-9)
    return 100 - (100 / (1 + rs))

def calculate_mom(series, period=10):
    return (series / series.shift(period) - 1) * 100

@st.cache_data(ttl=3600*12) 
def compute_indicators(ts_code, end_date):
    start_date = (datetime.strptime(end_date, "%Y%m%d") - timedelta(days=150)).strftime("%Y%m%d")
    df = get_qfq_data_v4_optimized_final(ts_code, start_date=start_date, end_date=end_date)
    res = {}
    if df.empty or len(df) < 26: return res 
    
    df['pct_chg'] = df['close'].pct_change().fillna(0) * 100 
    
    res['pct_lag1'] = df['pct_chg'].iloc[-2] if len(df) >= 2 else 0
    
    close = df['close']
    res['last_close'] = close.iloc[-1]
    res['last_open'] = df['open'].iloc[-1]
    res['last_high'] = df['high'].iloc[-1]
    res['last_low'] = df['low'].iloc[-1]
    
    ema12 = close.ewm(span=12, adjust=False).mean()
    ema26 = close.ewm(span=26, adjust=False).mean()
    diff = ema12 - ema26
    dea = diff.ewm(span=9, adjust=False).mean()
    res['macd_val'] = ((diff - dea) * 2).iloc[-1]
    
    res['ma5'] = close.tail(5).mean()
    res['ma20'] = close.tail(20).mean()
    
    rsi_series = calculate_rsi(close, period=12)
    res['rsi_12'] = rsi_series.iloc[-1]
    
    mom_series = calculate_mom(close, period=10)
    res['mom'] = mom_series.iloc[-1]
    
    return res

@st.cache_data(ttl=3600*12)
def get_market_state(trade_date):
    start_date = (datetime.strptime(trade_date, "%Y%m%d") - timedelta(days=40)).strftime("%Y%m%d")
    index_data = safe_get('daily', ts_code='000300.SH', start_date=start_date, end_date=trade_date, is_index=True)
    if index_data.empty or len(index_data) < 20: return 'Weak'
    index_data = index_data.sort_values('trade_date')
    latest_close = index_data.iloc[-1]['close']
    ma20 = index_data['close'].tail(20).mean()
    return 'Strong' if latest_close > ma20 else 'Weak'

# ---------------------------
# æ ¸å¿ƒå›æµ‹é€»è¾‘å‡½æ•° (V36.15 ç©¿è¶Šè€…)
# ---------------------------
def run_backtest_for_a_day(last_trade, TOP_BACKTEST, FINAL_POOL, MOM_LIMIT, MAX_TURNOVER_RATE, MIN_BODY_POS, RSI_LIMIT, CHIP_MIN_WIN_RATE, SECTOR_THRESHOLD, MIN_MV, MAX_MV, MAX_PREV_PCT, MIN_PRICE):
    global GLOBAL_STOCK_INDUSTRY
    
    market_state = get_market_state(last_trade)
    daily_all = safe_get('daily', trade_date=last_trade) 
    if daily_all.empty: return pd.DataFrame(), f"æ•°æ®ç¼ºå¤± {last_trade}"

    stock_basic = safe_get('stock_basic', list_status='L', fields='ts_code,name,list_date')
    if stock_basic.empty or 'name' not in stock_basic.columns:
        stock_basic = safe_get('stock_basic', list_status='L')
    
    chip_dict = {}
    try:
        chip_df = safe_get('cyq_perf', trade_date=last_trade)
        if not chip_df.empty:
            chip_dict = dict(zip(chip_df['ts_code'], chip_df['winner_rate']))
    except: pass 
    
    strong_industry_codes = set()
    try:
        sw_df = safe_get('sw_daily', trade_date=last_trade)
        if not sw_df.empty:
            strong_sw = sw_df[sw_df['pct_chg'] >= SECTOR_THRESHOLD]
            strong_industry_codes = set(strong_sw['index_code'].tolist())
    except: pass 
        
    df = daily_all.merge(stock_basic, on='ts_code', how='left')
    if 'name' not in df.columns: df['name'] = ''

    daily_basic = safe_get('daily_basic', trade_date=last_trade)
    if not daily_basic.empty:
        needed_cols = ['ts_code','turnover_rate','circ_mv','amount']
        existing_cols = [c for c in needed_cols if c in daily_basic.columns]
        df = df.merge(daily_basic[existing_cols], on='ts_code', how='left')
    
    mf_raw = safe_get('moneyflow', trade_date=last_trade)
    if not mf_raw.empty:
        mf = mf_raw[['ts_code','net_mf_amount']].rename(columns={'net_mf_amount':'net_mf'})
        df = df.merge(mf, on='ts_code', how='left')
    else:
        df['net_mf'] = 0 
    
    for col in ['net_mf', 'turnover_rate', 'circ_mv', 'amount']:
        if col not in df.columns: df[col] = 0
    df['net_mf'] = df['net_mf'].fillna(0)
    df['circ_mv_billion'] = df['circ_mv'] / 10000 
    
    df = df[~df['name'].str.contains('ST|é€€', na=False)]
    df = df[~df['ts_code'].str.startswith('92')] 
    
    df = df[(df['close'] >= MIN_PRICE) & (df['close'] <= 2000.0)]
    df = df[(df['circ_mv_billion'] >= MIN_MV) & (df['circ_mv_billion'] <= MAX_MV)]
    df = df[df['turnover_rate'] <= MAX_TURNOVER_RATE] 

    # æ²¿ç”¨ V36.14 çš„ç­›é€‰
    df = df[df['pct_chg'] > 4.5]
    df = df[df['pct_chg'] < 10.5] 

    if len(df) == 0: return pd.DataFrame(), "è¿‡æ»¤åæ— æ ‡çš„"

    candidates = df.sort_values('pct_chg', ascending=False).head(FINAL_POOL)
    records = []
    
    for row in candidates.itertuples():
        if GLOBAL_STOCK_INDUSTRY and strong_industry_codes:
            ind_code = GLOBAL_STOCK_INDUSTRY.get(row.ts_code)
            if ind_code and (ind_code not in strong_industry_codes): continue
        
        ind = compute_indicators(row.ts_code, last_trade)
        if not ind: continue
        d0_close = ind['last_close']
        d0_rsi = ind.get('rsi_12', 50)
        d0_mom = ind.get('mom', 0)
        d0_pct_lag1 = ind.get('pct_lag1', 0)
        
        # [V36.14 ç­›é€‰é€»è¾‘]
        win_rate = chip_dict.get(row.ts_code, 50) 
        if win_rate < 75.0: continue 
        
        if d0_mom < 7.0: continue 
        if d0_rsi < 50: continue
        if d0_pct_lag1 < -3.0: continue
        if (d0_close / ind['ma5']) > 1.12: continue
        
        if market_state == 'Weak':
            if d0_rsi > RSI_LIMIT: continue
            if d0_close < ind['ma20']: continue 
        
        range_len = ind['last_high'] - ind['last_low']
        if range_len > 0:
            body_pos = (d0_close - ind['last_low']) / range_len
            if body_pos < MIN_BODY_POS: continue 

        # [MODIFIED] ä½¿ç”¨å»¶æ—¶ä¹°å…¥å‡½æ•°
        # ä¼ å…¥ delay_days = 5 (å³ä¹°å…¥ T+5)
        future = get_future_prices_delayed(row.ts_code, last_trade, delay_days=5)
        
        records.append({
            'ts_code': row.ts_code, 'name': row.name, 'Close': row.close, 'Pct_Chg': row.pct_chg,
            'rsi': d0_rsi, 'mom': d0_mom, 'winner_rate': win_rate, 
            # æ³¨æ„: è¿™é‡Œçš„ returns æ˜¯ T+5 ä¹°å…¥åçš„æ”¶ç›Š
            'Return_Delay_1 (%)': future.get('Return_Delay_1', np.nan),
            'Return_Delay_3 (%)': future.get('Return_Delay_3', np.nan),
            'Return_Delay_5 (%)': future.get('Return_Delay_5', np.nan),
            'Sector_Boost': 'Yes' if GLOBAL_STOCK_INDUSTRY else 'N/A'
        })
            
    if not records: return pd.DataFrame(), "æ·±åº¦ç­›é€‰åæ— æ ‡çš„"
    fdf = pd.DataFrame(records)
    
    def dynamic_score(r):
        base_score = r['mom'] * 20 + (r['winner_rate'] * 10)
        if r['rsi'] > 85: base_score += 2000
        return base_score

    fdf['Score'] = fdf.apply(dynamic_score, axis=1)
    
    final_df = fdf.sort_values('Score', ascending=False).head(TOP_BACKTEST).copy()
    final_df.insert(0, 'Rank', range(1, len(final_df) + 1))
    
    return final_df, None

# ---------------------------
# UI åŠ ä¸»ç¨‹åº
# ---------------------------
with st.sidebar:
    st.header("V36.15 ç©¿è¶Šè€…")
    backtest_date_end = st.date_input("åˆ†ææˆªæ­¢æ—¥æœŸ", value=datetime.now().date())
    BACKTEST_DAYS = st.number_input("åˆ†æå¤©æ•°", value=30, step=1, help="å»ºè®®30-50å¤©")
    TOP_BACKTEST = st.number_input("æ¯æ—¥ä¼˜é€‰ TopK", value=4)
    
    st.markdown("---")
    RESUME_CHECKPOINT = st.checkbox("ğŸ”¥ å¼€å¯æ–­ç‚¹ç»­ä¼ ", value=True)
    if st.button("ğŸ—‘ï¸ æ¸…é™¤è¡Œæƒ…ç¼“å­˜"):
        if os.path.exists(CACHE_FILE_NAME):
            os.remove(CACHE_FILE_NAME)
            st.success("ç¼“å­˜å·²æ¸…é™¤ã€‚")
    CHECKPOINT_FILE = "backtest_checkpoint_v36_15.csv"
    
    st.markdown("---")
    st.subheader("âš”ï¸ å‚æ•°è®¾å®š")
    st.info("é€‰è‚¡é€»è¾‘åŒ V36.14")
    st.info("ä¹°å…¥é€»è¾‘: T+5 å¼€ç›˜ä¹°å…¥")

TS_TOKEN = st.text_input("Tushare Token", type="password")
if not TS_TOKEN: st.stop()
ts.set_token(TS_TOKEN)
pro = ts.pro_api()

if st.button(f"ğŸš€ å¯åŠ¨ V36.15"):
    processed_dates = set()
    results = []
    
    if RESUME_CHECKPOINT and os.path.exists(CHECKPOINT_FILE):
        try:
            existing_df = pd.read_csv(CHECKPOINT_FILE)
            existing_df['Trade_Date'] = existing_df['Trade_Date'].astype(str)
            processed_dates = set(existing_df['Trade_Date'].unique())
            results.append(existing_df)
            st.success(f"âœ… æ£€æµ‹åˆ°æ–­ç‚¹å­˜æ¡£ï¼Œè·³è¿‡ {len(processed_dates)} ä¸ªäº¤æ˜“æ—¥...")
        except:
            if os.path.exists(CHECKPOINT_FILE): os.remove(CHECKPOINT_FILE)
    else:
        if os.path.exists(CHECKPOINT_FILE): os.remove(CHECKPOINT_FILE)
    
    trade_days_list = get_trade_days(backtest_date_end.strftime("%Y%m%d"), int(BACKTEST_DAYS))
    if not trade_days_list: st.stop()
        
    dates_to_run = [d for d in trade_days_list if d not in processed_dates]
    
    if not dates_to_run:
        st.success("ğŸ‰ æ‰€æœ‰æ—¥æœŸå·²è®¡ç®—å®Œæ¯•ï¼")
    else:
        if not get_all_historical_data(trade_days_list, use_cache=True):
            st.stop()
            
        bar = st.progress(0, text="å›æµ‹å¼•æ“å¯åŠ¨...")
        
        for i, date in enumerate(dates_to_run):
            # æ³¨æ„: ä½¿ç”¨ run_backtest_for_a_day (V36.15)
            # å‚æ•°æ²¿ç”¨ V36.14 çš„é»˜è®¤å€¼
            res, err = run_backtest_for_a_day(date, int(TOP_BACKTEST), 100, 7.0, 20.0, 0.6, 100.0, 75.0, 1.0, 30.0, 1000.0, 999, 15.0)
            if not res.empty:
                res['Trade_Date'] = date
                is_first = not os.path.exists(CHECKPOINT_FILE)
                res.to_csv(CHECKPOINT_FILE, mode='a', index=False, header=is_first, encoding='utf-8-sig')
                results.append(res)
            
            bar.progress((i+1)/len(dates_to_run), text=f"åˆ†æä¸­: {date}")
        
        bar.empty()
    
    if results:
        all_res = pd.concat(results)
        all_res = all_res[all_res['Rank'] <= int(TOP_BACKTEST)]
        all_res['Trade_Date'] = all_res['Trade_Date'].astype(str)
        all_res = all_res.sort_values(['Trade_Date', 'Rank'], ascending=[False, True])
        
        st.header(f"ğŸ“Š V36.15 å»¶æ—¶ä¹°å…¥ç»Ÿè®¡ (Top {TOP_BACKTEST})")
        cols = st.columns(3)
        for idx, n in enumerate([1, 3, 5]):
            col_name = f'Return_Delay_{n} (%)' # æ³¨æ„åˆ—åå˜äº†
            valid = all_res.dropna(subset=[col_name]) 
            if not valid.empty:
                avg = valid[col_name].mean()
                win = (valid[col_name] > 0).mean() * 100
                cols[idx].metric(f"å»¶æ—¶+{n} å‡ç›Š / èƒœç‡", f"{avg:.2f}% / {win:.1f}%")
 
        st.subheader("ğŸ“‹ å›æµ‹æ¸…å•")
        
        # æ˜¾ç¤ºå»¶æ—¶å›æŠ¥åˆ—
        show_cols = ['Rank', 'Trade_Date','name','ts_code','Close','Pct_Chg',
             'Return_Delay_1 (%)', 'Return_Delay_3 (%)', 'Return_Delay_5 (%)',
                        'rsi','mom','winner_rate']
        final_cols = [c for c in show_cols if c in all_res.columns]
    
        st.dataframe(all_res[final_cols], use_container_width=True)
        
        csv = all_res.to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ“¥ ä¸‹è½½ç»“æœ (CSV)", csv, f"export.csv", "text/csv")
    else:
        st.warning("âš ï¸ æ²¡æœ‰ç»“æœã€‚")
