# -*- coding: utf-8 -*-
"""
é€‰è‚¡ç‹ Â· V30.24 é€»è¾‘å›å½’ç‰ˆ (å…¨æ’åè§£é” + å¤§ç›˜é£æ§)
æ ¸å¿ƒç†å¿µï¼š
1. [çº¯ç²¹] ç§»é™¤æ‰€æœ‰äººå·¥åŠ åˆ†(æ¶¨åœ/æ³¢åŠ¨ç‡)ï¼Œå›å½’ MACD/Price çº¯ç²¹å¼ºåº¦ã€‚
2. [å¹¿åº¦] ä»·æ ¼æ”¾å®½è‡³ 20-300å…ƒï¼Œå–æ¶ˆ"å…¥å›´æ•°é‡"é™åˆ¶ï¼Œå…¨å¸‚åœºæ‰«æã€‚
3. [é£æ§] å¼•å…¥å¤§ç›˜(ä¸Šè¯æŒ‡æ•°)MA20è¿‡æ»¤ï¼Œå¤§ç›˜èµ°åæ—¶è‡ªåŠ¨ç©ºä»“ã€‚
4. [éªŒè¯] è§£é” Top 5 å®Œæ•´æŠ¥å‘Šï¼ŒéªŒè¯ç­–ç•¥çš„çº¿æ€§è¡°å‡é€»è¾‘ã€‚
"""

import streamlit as st
import pandas as pd
import numpy as np
import tushare as ts
from datetime import datetime, timedelta
import time

# ---------------------------
# é¡µé¢é…ç½®
# ---------------------------
st.set_page_config(page_title="é€‰è‚¡ç‹ Â· V30.24 é€»è¾‘å›å½’ç‰ˆ", layout="wide")
st.title("é€‰è‚¡ç‹ Â· V30.24 é€»è¾‘å›å½’ç‰ˆ (ğŸ›¡ï¸ å¤§ç›˜é£æ§ + ğŸ¯ çº¯ç²¹åŠ¨é‡ + ğŸ”“ å…¨æ’å)")
st.markdown("""
**ğŸ“ ç­–ç•¥é€»è¾‘é‡æ„ï¼š**
1. **æµ·é€‰æ±  (å®½è¿›)ï¼š** ä»·æ ¼ `20-300å…ƒ` + æµé€šå¸‚å€¼ `>30äº¿` + å‰”é™¤ST/ä¸€å­—æ¿ã€‚
2. **è¯„åˆ† (çº¯ç²¹)ï¼š** ä»…ä½¿ç”¨ `(MACD / è‚¡ä»·)` è¡¡é‡ç›¸å¯¹å¼ºåº¦ã€‚æ— äººå·¥åŠ åˆ†ï¼ŒRank 1 å³æœ€å¼ºã€‚
3. **æ‹©æ—¶ (æ–°å¢)ï¼š** ğŸ›¡ï¸ **å¤§ç›˜é£æ§**ï¼šè‹¥ä¸Šè¯æŒ‡æ•°è·Œç ´ 20æ—¥çº¿ï¼Œå½“æ—¥**å¼ºåˆ¶ç©ºä»“**ï¼Œä¿ä½åˆ©æ¶¦ã€‚
4. **ç›®æ ‡ï¼š** å¯»æ‰¾çœŸå®çš„ Alphaï¼Œä¸ä¾èµ–ç‰¹å®šå‚æ•°è¿‡æ‹Ÿåˆã€‚
""")

# ---------------------------
# å…¨å±€ç¼“å­˜
# ---------------------------
pro = None 
GLOBAL_ADJ_FACTOR = pd.DataFrame() 
GLOBAL_DAILY_RAW = pd.DataFrame() 
GLOBAL_QFQ_BASE_FACTORS = {} 
GLOBAL_INDEX_DATA = pd.DataFrame() # ç¼“å­˜å¤§ç›˜æ•°æ®

# ---------------------------
# åŸºç¡€å·¥å…·å‡½æ•°
# ---------------------------
@st.cache_data(ttl=3600*12) 
def safe_get(func_name, **kwargs):
    global pro
    if pro is None: return pd.DataFrame(columns=['ts_code']) 
    func = getattr(pro, func_name) 
    try:
        if kwargs.get('is_index'): df = pro.index_daily(**kwargs)
        else: df = func(**kwargs)
        if df is None or (isinstance(df, pd.DataFrame) and df.empty):
            return pd.DataFrame(columns=['ts_code']) 
        return df
    except Exception: return pd.DataFrame(columns=['ts_code'])

def get_trade_days(end_date_str, num_days):
    # å¤šå–ä¸€äº›å¤©æ•°ä»¥è®¡ç®—æŒ‡æ ‡
    start_date = (datetime.strptime(end_date_str, "%Y%m%d") - timedelta(days=num_days * 5)).strftime("%Y%m%d")
    cal = safe_get('trade_cal', start_date=start_date, end_date=end_date_str)
    if cal.empty or 'is_open' not in cal.columns:
        st.error("æ— æ³•è·å–äº¤æ˜“æ—¥å†ã€‚")
        return []
    return cal[cal['is_open'] == 1].sort_values('cal_date', ascending=False)['cal_date'].head(num_days).tolist()

# ----------------------------------------------------------------------
# æ•°æ®ä¸‹è½½ (å¢åŠ å¤§ç›˜æ•°æ®)
# ----------------------------------------------------------------------
@st.cache_data(ttl=3600*24)
def fetch_and_cache_daily_data(date):
    adj_df = safe_get('adj_factor', trade_date=date)
    daily_df = safe_get('daily', trade_date=date)
    return {'adj': adj_df, 'daily': daily_df}

def get_all_historical_data(trade_days_list):
    global GLOBAL_ADJ_FACTOR, GLOBAL_DAILY_RAW, GLOBAL_QFQ_BASE_FACTORS, GLOBAL_INDEX_DATA
    if not trade_days_list: return False
    
    latest_trade_date = max(trade_days_list) 
    earliest_trade_date = min(trade_days_list)
    
    # 1. ä¸‹è½½å¤§ç›˜æ•°æ® (ä¸Šè¯æŒ‡æ•°) ç”¨äºé£æ§
    start_date_idx = (datetime.strptime(earliest_trade_date, "%Y%m%d") - timedelta(days=60)).strftime("%Y%m%d")
    end_date_idx = (datetime.strptime(latest_trade_date, "%Y%m%d") + timedelta(days=10)).strftime("%Y%m%d")
    
    with st.spinner("æ­£åœ¨è·å–å¤§ç›˜æŒ‡æ•°æ•°æ®..."):
        GLOBAL_INDEX_DATA = safe_get('index_daily', ts_code='000001.SH', start_date=start_date_idx, end_date=end_date_idx)
        if not GLOBAL_INDEX_DATA.empty:
            GLOBAL_INDEX_DATA = GLOBAL_INDEX_DATA.sort_values('trade_date').set_index('trade_date')
            # è®¡ç®—å¤§ç›˜ MA20
            GLOBAL_INDEX_DATA['ma20'] = GLOBAL_INDEX_DATA['close'].rolling(window=20).mean()

    # 2. ä¸‹è½½ä¸ªè‚¡æ•°æ®
    start_date = (datetime.strptime(earliest_trade_date, "%Y%m%d") - timedelta(days=150)).strftime("%Y%m%d")
    end_date = (datetime.strptime(latest_trade_date, "%Y%m%d") + timedelta(days=25)).strftime("%Y%m%d") 
    
    all_dates = safe_get('trade_cal', start_date=start_date, end_date=end_date, is_open='1')['cal_date'].tolist()
    st.info(f"â³ æ­£åœ¨æŒ‰æ—¥æœŸåœ°æ¯¯å¼ä¸‹è½½ {start_date} åˆ° {end_date} å…¨å¸‚åœºæ•°æ®...")

    adj_list, daily_list = [], []
    download_progress = st.progress(0, text="ä¸‹è½½è¿›åº¦...")
    
    # ä¼˜åŒ–ï¼šåˆ†æ‰¹æ¬¡æˆ–ç›´æ¥ä¸‹è½½å¯èƒ½å¤ªæ…¢ï¼Œç»´æŒé€æ—¥ä¸‹è½½ä½†ç¡®ä¿å®Œæ•´æ€§
    total_dates = len(all_dates)
    for i, date in enumerate(all_dates):
        try:
            cached_data = fetch_and_cache_daily_data(date)
            if not cached_data['adj'].empty: adj_list.append(cached_data['adj'])
            if not cached_data['daily'].empty: daily_list.append(cached_data['daily'])
            if i % 5 == 0: # å‡å°‘åˆ·æ–°é¢‘ç‡æå‡é€Ÿåº¦
                download_progress.progress((i + 1) / total_dates)
        except: continue 
    download_progress.empty()

    if not adj_list or not daily_list:
        st.error("æ— æ³•è·å–å†å²æ•°æ®ï¼Œè¯·æ£€æŸ¥Tokenæˆ–ç½‘ç»œã€‚")
        return False
        
    adj_data = pd.concat(adj_list)
    adj_data['adj_factor'] = pd.to_numeric(adj_data['adj_factor'], errors='coerce').fillna(0)
    GLOBAL_ADJ_FACTOR = adj_data.set_index(['ts_code', 'trade_date']).sort_index(level=[0, 1]) 
    
    cols_to_keep = ['ts_code', 'trade_date', 'open', 'high', 'low', 'close', 'pre_close', 'vol']
    valid_cols = [c for c in cols_to_keep if c in daily_list[0].columns]
    daily_raw = pd.concat(daily_list)[valid_cols]
    
    for col in ['open', 'high', 'low', 'close', 'pre_close', 'vol']:
        if col in daily_raw.columns:
            daily_raw[col] = pd.to_numeric(daily_raw[col], errors='coerce').astype('float32')

    GLOBAL_DAILY_RAW = daily_raw.set_index(['ts_code', 'trade_date']).sort_index(level=[0, 1])

    latest_global_date = GLOBAL_ADJ_FACTOR.index.get_level_values('trade_date').max()
    if latest_global_date:
        try:
            latest_adj = GLOBAL_ADJ_FACTOR.loc[(slice(None), latest_global_date), 'adj_factor']
            GLOBAL_QFQ_BASE_FACTORS = latest_adj.droplevel(1).to_dict()
        except: GLOBAL_QFQ_BASE_FACTORS = {}
    
    return True

# ----------------------------------------------------------------------
# å¤æƒæ•°æ®æå–
# ----------------------------------------------------------------------
def get_qfq_data_v4(ts_code, start_date, end_date):
    global GLOBAL_DAILY_RAW, GLOBAL_ADJ_FACTOR, GLOBAL_QFQ_BASE_FACTORS
    if GLOBAL_DAILY_RAW.empty or GLOBAL_ADJ_FACTOR.empty: return pd.DataFrame()
        
    base_adj = GLOBAL_QFQ_BASE_FACTORS.get(ts_code, np.nan)
    if pd.isna(base_adj) or base_adj < 1e-9: return pd.DataFrame() 

    try:
        # å¿«é€Ÿåˆ‡ç‰‡
        daily = GLOBAL_DAILY_RAW.loc[(ts_code, slice(start_date, end_date)), :]
        adj = GLOBAL_ADJ_FACTOR.loc[(ts_code, slice(start_date, end_date)), 'adj_factor']
    except KeyError: return pd.DataFrame()
    
    if daily.empty or adj.empty: return pd.DataFrame()
    
    # ç´¢å¼•å¯¹é½
    df = daily.join(adj, how='left').dropna(subset=['adj_factor'])
    
    factor = df['adj_factor'] / base_adj
    for col in ['open', 'high', 'low', 'close', 'pre_close']:
        if col in df.columns: df[col] = df[col] * factor
    
    df = df.reset_index()
    df['trade_date'] = pd.to_datetime(df['trade_date'], format='%Y%m%d')
    return df.set_index('trade_date').sort_index()[['open', 'high', 'low', 'close', 'pre_close', 'vol']]

# ----------------------------------------------------------------------
# æ ¸å¿ƒæŒ‡æ ‡è®¡ç®—
# ----------------------------------------------------------------------
@st.cache_data(ttl=3600*12) 
def compute_indicators(ts_code, end_date):
    # å‘å‰å–120å¤©è¶³å¤Ÿè®¡ç®—MACD
    start_date = (datetime.strptime(end_date, "%Y%m%d") - timedelta(days=120)).strftime("%Y%m%d")
    df = get_qfq_data_v4(ts_code, start_date=start_date, end_date=end_date)
    res = {}
    if df.empty or len(df) < 26: return res
         
    close = df['close']
    vol = df['vol']
    
    # 1. æš´åŠ› MACD (8, 17, 5) - çµæ•åº¦é«˜ï¼Œé€‚åˆæ•æ‰èµ·æ¶¨
    ema_fast = close.ewm(span=8, adjust=False).mean()
    ema_slow = close.ewm(span=17, adjust=False).mean()
    diff = ema_fast - ema_slow
    dea = diff.ewm(span=5, adjust=False).mean()
    macd_val = (diff - dea) * 2
    
    res['macd_val'] = macd_val.iloc[-1]
    
    # 2. å‡çº¿ä¸é‡èƒ½
    ma20 = close.rolling(window=20).mean()
    ma5_vol = vol.rolling(window=5).mean()
    
    res['close_current'] = close.iloc[-1]
    res['ma20_current'] = ma20.iloc[-1] if not pd.isna(ma20.iloc[-1]) else 0
    res['vol_current'] = vol.iloc[-1]
    res['ma5_vol_current'] = ma5_vol.iloc[-1] if not pd.isna(ma5_vol.iloc[-1]) else 0
    
    return res

# ----------------------------------------------------------------------
# æœªæ¥æ”¶ç›Šè®¡ç®—
# ----------------------------------------------------------------------
def get_future_returns(ts_code, selection_date, buy_threshold_pct=1.5):
    d0 = datetime.strptime(selection_date, "%Y%m%d")
    start_future = (d0 + timedelta(days=1)).strftime("%Y%m%d")
    end_future = (d0 + timedelta(days=20)).strftime("%Y%m%d")
    
    hist = get_qfq_data_v4(ts_code, start_date=start_future, end_date=end_future)
    results = {'Return_D1': np.nan, 'Return_D3': np.nan, 'Return_D5': np.nan}

    if hist.empty: return results
    d1_data = hist.iloc[0]
    
    # å®æˆ˜æ¨¡æ‹Ÿï¼šæ‹’ç»ä½å¼€
    if d1_data['open'] <= d1_data['pre_close']: return results 
    
    # å®æˆ˜æ¨¡æ‹Ÿï¼šç›˜ä¸­å¿…é¡»è§¦åŠ +1.5% æ‰èƒ½æˆäº¤
    buy_price = d1_data['open'] * (1 + buy_threshold_pct / 100.0)
    if d1_data['high'] < buy_price: return results 

    # è®¡ç®—æ”¶ç›Š
    for n in [1, 3, 5]:
        idx = n - 1
        if len(hist) > idx:
            # æ”¶ç›Š = (Næ—¥åæ”¶ç›˜ä»· - ä¹°å…¥ä»·) / ä¹°å…¥ä»·
            results[f'Return_D{n}'] = (hist.iloc[idx]['close'] / buy_price - 1) * 100
            
    return results

# ----------------------------------------------------
# ä¾§è¾¹æ è®¾ç½®
# ----------------------------------------------------
with st.sidebar:
    st.header("1. å›æµ‹å‚æ•°")
    backtest_date_end = st.date_input("ç»“æŸæ—¥æœŸ", value=datetime.now().date(), max_value=datetime.now().date())
    BACKTEST_DAYS = int(st.number_input("å›æµ‹å¤©æ•°", value=50, step=1))
    
    st.markdown("---")
    st.header("2. é€‰è‚¡é—¨æ§› (V30.24)")
    # å“åº”å»ºè®®ï¼šä»·æ ¼åŒºé—´æ”¾å®½ï¼Œç”¨å¸‚å€¼è¿‡æ»¤åƒåœ¾è‚¡
    MIN_PRICE = st.number_input("æœ€ä½è‚¡ä»·", value=20.0, step=1.0) 
    MAX_PRICE = st.number_input("æœ€é«˜è‚¡ä»·", value=300.0, step=5.0)
    MIN_CIRC_MV = st.number_input("æœ€ä½æµé€šå¸‚å€¼(äº¿)", value=30.0, step=5.0) # 30äº¿èµ·ï¼Œé¿å¼€å¾®ç›˜
    BUY_THRESHOLD = st.number_input("ä¹°å…¥è§¦å‘æ¶¨å¹…(%)", value=1.5)

    st.markdown("---")
    st.info("âš ï¸ æ³¨æ„ï¼šæœ¬ç‰ˆæœ¬å¼€å¯äº†å¤§ç›˜é£æ§ã€‚è‹¥ä¸Šè¯æŒ‡æ•°è·Œç ´20æ—¥çº¿ï¼Œå½“å¤©å°†ä¸ä¼šä¹°å…¥ä»»ä½•è‚¡ç¥¨ã€‚")

TS_TOKEN = st.text_input("Tushare Token", type="password")
if not TS_TOKEN: st.stop()
ts.set_token(TS_TOKEN)
pro = ts.pro_api() 

# ----------------------------------------------------------------------
# V30.24 æ ¸å¿ƒé€»è¾‘ï¼šå…¨æ‰«æ + çº¯ç²¹è¯„åˆ† + å¤§ç›˜é£æ§
# ----------------------------------------------------------------------
def run_backtest_daily(date_str):
    # 1. å¤§ç›˜é£æ§ (The Great Filter)
    if not GLOBAL_INDEX_DATA.empty and date_str in GLOBAL_INDEX_DATA.index:
        idx_today = GLOBAL_INDEX_DATA.loc[date_str]
        # å¦‚æœæ”¶ç›˜ä»· < 20æ—¥å‡çº¿ï¼Œåˆ¤å®šä¸ºå¼±åŠ¿ï¼Œç©ºä»“
        if idx_today['close'] < idx_today['ma20']:
            return pd.DataFrame(), "ğŸ›¡ï¸ å¤§ç›˜ç ´ä½(MA20)ï¼Œç³»ç»Ÿç©ºä»“é¿é™©"
    
    # 2. è·å–å½“æ—¥å…¨å¸‚åœºæ•°æ®
    daily = safe_get('daily', trade_date=date_str)
    if daily.empty: return pd.DataFrame(), "æ•°æ®ç¼ºå¤±"
    
    # 3. åŸºç¡€è¿‡æ»¤ (Fast Filter)
    pool = daily.copy()
    pool['close'] = pd.to_numeric(pool['close'], errors='coerce')
    
    # è·å–å¸‚å€¼æ•°æ®
    d_basic = safe_get('daily_basic', trade_date=date_str, fields='ts_code,circ_mv,turnover_rate')
    if d_basic.empty: return pd.DataFrame(), "åŸºç¡€æ•°æ®ç¼ºå¤±"
    pool = pool.merge(d_basic, on='ts_code', how='inner')
    
    # 3.1 ä»·æ ¼è¿‡æ»¤ (20 - 300)
    pool = pool[(pool['close'] >= MIN_PRICE) & (pool['close'] <= MAX_PRICE)]
    
    # 3.2 å¸‚å€¼è¿‡æ»¤ (> 30äº¿, å•ä½æ˜¯ä¸‡) -> 300000ä¸‡å…ƒ
    pool['circ_mv_billion'] = pool['circ_mv'] / 10000 
    pool = pool[pool['circ_mv_billion'] >= MIN_CIRC_MV]
    
    # 3.3 å‰”é™¤ ST å’Œ åŒ—äº¤æ‰€
    pool = pool[~pool['ts_code'].str.startswith(('8', '4', '92'))] 
    
    # 3.4 å‰”é™¤ä¸€å­—æ¿ (High == Low ä¸” æ¶¨å¹… > 9%) - æ ¸å¿ƒé˜²å‘
    pool = pool[~((pool['high'] == pool['low']) & (pool['pct_chg'] > 9.0))]

    # 3.5 [V30.24å…³é”®] å…¨æ‰«ææ¨¡å¼ï¼Œä¸é™åˆ¶"å‰100å"
    # ä½†ä¸ºäº†ä¸è¶…æ—¶ï¼Œæˆ‘ä»¬è‡³å°‘è¦æ±‚æ˜¯"ä¸Šæ¶¨çš„" (Pct_Chg > 0)
    candidates = pool[pool['pct_chg'] > 0]
    
    if len(candidates) > 400:
        # å¦‚æœå€™é€‰å¤ªå¤šï¼Œä¼˜å…ˆç®—æ¶¨å¹…å‰400å (ç®—åŠ›å¦¥åï¼Œä½†æ¯”å‰100å®½å¤šäº†)
        candidates = candidates.sort_values('pct_chg', ascending=False).head(400)
    
    if candidates.empty: return pd.DataFrame(), "æ— ç¬¦åˆåˆé€‰è‚¡ç¥¨"

    # 4. ç²¾ç»†è®¡ç®— (MACD)
    records = []
    
    for row in candidates.itertuples():
        ind = compute_indicators(row.ts_code, date_str)
        
        # æ ¸å¿ƒæ¡ä»¶ï¼š
        if ind.get('close_current', 0) <= ind.get('ma20_current', 0): continue
        if ind.get('vol_current', 0) <= ind.get('ma5_vol_current', 0) * 1.2: continue
        if pd.isna(ind.get('macd_val')) or ind.get('macd_val') <= 0: continue
        
        # æ»¡è¶³æ¡ä»¶ï¼Œè®¡ç®—æœªæ¥æ”¶ç›Š
        future = get_future_returns(row.ts_code, date_str, buy_threshold_pct=BUY_THRESHOLD)
        
        # è¯„åˆ†ï¼šçº¯ç²¹çš„ç›¸å¯¹å¼ºåº¦ (MACD / Price)
        score = (ind['macd_val'] / row.close) * 100000
        
        records.append({
            'ts_code': row.ts_code,
            'Close': row.close,
            'Pct_Chg': row.pct_chg,
            'MACD': ind['macd_val'],
            'Score': score,
            'Return_D1': future['Return_D1'],
            'Return_D3': future['Return_D3'],
            'Return_D5': future['Return_D5']
        })
    
    if not records: return pd.DataFrame(), "æ— MACDè¾¾æ ‡è‚¡ç¥¨"
    
    # 5. æ’åºä¸è¾“å‡º
    df_res = pd.DataFrame(records)
    df_res = df_res.sort_values('Score', ascending=False).head(5) # åªè¾“å‡ºå‰5å
    
    return df_res, "Success"

# ---------------------------
# ä¸»ç¨‹åº
# ---------------------------
if st.button(f"ğŸš€ è¿è¡Œ V30.24 (é€»è¾‘å›å½’ + é£æ§)"):
    trade_days = get_trade_days(backtest_date_end.strftime("%Y%m%d"), BACKTEST_DAYS)
    if not trade_days: st.stop()
    if not get_all_historical_data(trade_days): st.stop()
    
    st.success("âœ… V30.24 å¯åŠ¨ï¼šå…¨å¸‚åœºæ‰«æ | çº¯ç²¹è¯„åˆ† | å¤§ç›˜é£æ§")
    results = []
    bar = st.progress(0)
    status_text = st.empty()
    
    for i, date in enumerate(trade_days):
        status_text.text(f"æ­£åœ¨åˆ†æ: {date} ...")
        try:
            df, msg = run_backtest_daily(date)
            if not df.empty:
                df['Trade_Date'] = date
                # é‡æ–°è®¡ç®— Rank (1-5)
                df['Rank'] = range(1, len(df) + 1)
                results.append(df)
        except Exception as e:
            pass
        bar.progress((i + 1) / len(trade_days))
    
    bar.empty()
    status_text.text("å›æµ‹å®Œæˆï¼")
    
    if not results:
        st.warning("åŒºé—´å†…æ— äº¤æ˜“æˆ–å¤§ç›˜ä¸€ç›´å¤„äºé¿é™©çŠ¶æ€ã€‚")
        st.stop()
        
    all_res = pd.concat(results)
    if all_res['Trade_Date'].dtype != 'object': all_res['Trade_Date'] = all_res['Trade_Date'].astype(str)
        
    st.header(f"ğŸ“Š V30.24 å…¨æ’åå¯¹æ¯”æŠ¥å‘Š")
    st.markdown("é€šè¿‡å¯¹æ¯” Rank 1 åˆ° Rank 5 çš„è¡¨ç°ï¼ŒéªŒè¯ç­–ç•¥é€»è¾‘çš„çº¯ç²¹æ€§ä¸çº¿æ€§åº¦ã€‚")
    
    # 1. æ€»ä½“ç»Ÿè®¡
    rank1_df = all_res[all_res['Rank'] == 1]
    total_signals = len(rank1_df)
    valid_days = rank1_df['Return_D1'].notnull().sum()
    st.caption(f"å¤§ç›˜é£æ§åäº§ç”Ÿä¿¡å·å¤©æ•°ï¼š{total_signals} å¤© | å®æˆ˜æˆäº¤å¤©æ•°ï¼š{valid_days} å¤©")

    # 2. åˆ†æ’åè¯¦ç»†ç»Ÿè®¡
    results_list = []
    chart_data = pd.DataFrame() # ç”¨äºç”»å›¾
    
    for r in range(1, 6):
        # æå–å¯¹åº”æ’åçš„å­é›†
        df_r = all_res[all_res['Rank'] == r]
        
        # å‡†å¤‡ç”»å›¾æ•°æ®
        daily_ret = df_r.set_index('Trade_Date')['Return_D1'].groupby(level=0).mean()
        # å¡«å……ç©ºäº¤æ˜“æ—¥ä¸º0
        full_idx = pd.to_datetime(rank1_df['Trade_Date'].unique()).sort_values()
        # æ³¨æ„è¿™é‡Œç®€å•å¤„ç†ï¼šç”¨æ‰€æœ‰äº§ç”Ÿä¿¡å·çš„æ—¥å­åšè½´ï¼Œæœªæˆäº¤çš„æ—¥æ”¶ç›Šä¸º0
        daily_ret = daily_ret.reindex(full_idx.astype(str), fill_value=0)
        
        # ç´¯ç§¯æ”¶ç›Šæ›²çº¿ (ç®€å•å•åˆ©ç´¯åŠ æˆ–å¤åˆ©ï¼Œè¿™é‡Œç”¨å¤åˆ©)
        equity_curve = (1 + daily_ret.fillna(0)/100).cumprod()
        chart_data[f'Rank {r}'] = equity_curve
        
        # è®¡ç®— D+1 èƒœç‡å’Œæ”¶ç›Š
        valid_trades = df_r.dropna(subset=['Return_D1'])
        count = len(valid_trades)
        
        if count > 0:
            avg_ret = valid_trades['Return_D1'].mean()
            win_rate = (valid_trades['Return_D1'] > 0).sum() / count * 100
            
            # ç®€å•ä¼°ç®—å¹´åŒ–
            total_ret = equity_curve.iloc[-1] - 1
            if not equity_curve.empty:
                mdd = (equity_curve - equity_curve.cummax()) / equity_curve.cummax()
                max_dd = mdd.min()
            else:
                max_dd = 0
        else:
            avg_ret, win_rate, total_ret, max_dd = 0, 0, 0, 0

        results_list.append({
            'æ’å (Rank)': f"ç¬¬ {r} å",
            'D+1 å‡æ”¶': f"{avg_ret:.2f}%",
            'D+1 èƒœç‡': f"{win_rate:.1f}%",
            'ç´¯è®¡æ”¶ç›Š': f"{total_ret:.2%}",
            'æœ€å¤§å›æ’¤': f"{max_dd:.2%}",
            'æˆäº¤ç¬”æ•°': count
        })

    # 3. å±•ç¤ºè¡¨æ ¼
    st.table(pd.DataFrame(results_list))

    # 4. å¯è§†åŒ–å¯¹æ¯”
    st.subheader("ğŸ“ˆ åˆ†æ’åæ”¶ç›Šç‡æ›²çº¿å¯¹æ¯” (D+1)")
    st.line_chart(chart_data)

    st.header("ğŸ“‹ æ¯æ—¥è¯¦ç»†æ’å (Top 5)")
    st.dataframe(all_res.sort_values(['Trade_Date', 'Rank'], ascending=[False, True]), use_container_width=True)
