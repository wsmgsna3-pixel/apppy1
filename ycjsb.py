# -*- coding: utf-8 -*-
"""
ç¬¬ä¸€å 3.1 è‡ªç”±ç‰ˆ (è§£é”TopK + é˜²å¡æ­»ä¼˜åŒ–)
æ ¸å¿ƒå‡çº§ï¼š
1. [è‡ªç”±è®¾ç½®] è§£é™¤ Top 1 é”å®šï¼Œæ‚¨å¯ä»¥è‡ªç”±å›æµ‹ Top 3ã€Top 5 ç­‰ç»„åˆã€‚
2. [æ‹’ç»å¡æ­»] æ‰¹æ¬¡å¤§å° (Batch Size) é»˜è®¤ä¸‹è°ƒè‡³ 15ï¼Œé€‚é…äº‘ç«¯å†…å­˜ï¼Œé˜²æ­¢è¿›åº¦æ¡å‡æ­»ã€‚
3. [æé€Ÿå†…æ ¸] ä¿æŒå‘é‡åŒ–è®¡ç®—é€»è¾‘ï¼Œ500å¤©å›æµ‹ä¾ç„¶ä¸æ»‘ã€‚
"""

import streamlit as st
import pandas as pd
import numpy as np
import tushare as ts
from datetime import datetime, timedelta
import gc
import time
import os
import warnings
warnings.filterwarnings("ignore")

# ---------------------------
# å…¨å±€è®¾ç½®
# ---------------------------
st.set_page_config(page_title="ç¬¬ä¸€å 3.1 è‡ªç”±ç‰ˆ", layout="wide")

if 'pro' not in st.session_state:
    st.session_state.pro = None
if 'ts_token' not in st.session_state:
    st.session_state.ts_token = ""

# ---------------------------
# ç•Œé¢å¸ƒå±€
# ---------------------------
st.title("âš¡ ç¬¬ä¸€å 3.1 è‡ªç”±ç‰ˆ (Top K è§£é” + é˜²å¡æ­»)")
st.caption("ğŸš€ 500å¤©é•¿å‘¨æœŸå›æµ‹ä¸“ç”¨ | å‘é‡åŒ–æé€Ÿå†…æ ¸ | å†…å­˜é˜²çˆ†ä¼˜åŒ–")

with st.container():
    col1, col2 = st.columns([3, 1])
    with col1:
        new_token = st.text_input("ğŸ’ Tushare Token", value=st.session_state.ts_token, type="password")
        if new_token:
            st.session_state.ts_token = new_token
            ts.set_token(new_token)
            st.session_state.pro = ts.pro_api()
    with col2:
        st.write("")
        st.write("")
        start_btn = st.button("ğŸš€ æé€Ÿå›æµ‹", type="primary", use_container_width=True)

# å‚æ•°åŒºåŸŸ (é»˜è®¤å±•å¼€)
with st.expander("âš™ï¸ ç­–ç•¥å‚æ•°è®¾ç½®", expanded=True):
    c1, c2, c3 = st.columns(3)
    with c1:
        backtest_days = st.number_input("å›æµ‹å¤©æ•°", value=500, step=50, help="å»ºè®®500å¤©ä»¥è¦†ç›–ç‰›ç†Š")
        stop_loss_pct = st.number_input("æ­¢æŸé˜ˆå€¼ (%)", value=-4.0, step=0.5, help="ç›˜ä¸­è§¦åŠå³æ­¢æŸï¼Œå¡«è´Ÿæ•°")
    with c2:
        min_price = st.number_input("æœ€ä½è‚¡ä»·", value=40.0)
        max_price = st.number_input("æœ€é«˜è‚¡ä»·", value=300.0)
    with c3:
        buy_threshold = st.number_input("ä¹°å…¥é˜ˆå€¼ (%)", value=1.5)
        # [ä¿®æ”¹ç‚¹] è§£é” Top Kï¼Œå…è®¸ç”¨æˆ·è‡ªç”±è¾“å…¥ï¼Œé»˜è®¤æ”¹ä¸º 3 ä»¥æµ‹è¯•ç»„åˆæ•ˆæœ
        top_k = st.number_input("æ¯æ—¥æŒä»“ (Top K)", value=3, min_value=1, max_value=20, disabled=False, help="è®¾ç½®ä¸º 1 å³åªä¹°ç¬¬ä¸€åï¼Œè®¾ç½®ä¸º 3 å³ä¹°å‰ä¸‰å")

# ---------------------------
# æ ¸å¿ƒå¼•æ“ (å‘é‡åŒ–)
# ---------------------------
def get_trade_days(end_date_str, num_days):
    # æ‰©å¤§è·å–èŒƒå›´ä»¥åº”å¯¹èŠ‚å‡æ—¥
    start_date = (datetime.strptime(end_date_str, "%Y%m%d") - timedelta(days=num_days * 3 + 300)).strftime("%Y%m%d")
    if st.session_state.pro:
        try:
            cal = st.session_state.pro.trade_cal(start_date=start_date, end_date=end_date_str, is_open='1')
            return cal.sort_values('cal_date', ascending=False)['cal_date'].head(num_days).tolist()
        except: return []
    return []

def load_data_and_compute_vectorized(date_list):
    """
    [æ ¸å¿ƒé»‘ç§‘æŠ€] å‘é‡åŒ–è®¡ç®—å¼•æ“
    """
    if not date_list: return None
    
    # 1. ç¡®å®šæ•°æ®èŒƒå›´
    start_date = min(date_list)
    end_date = max(date_list)
    # ç¼“å†²æœŸéœ€è¶³å¤Ÿé•¿ä»¥ä¿è¯MACDå‡†ç¡®
    buffer_start = (datetime.strptime(start_date, "%Y%m%d") - timedelta(days=150)).strftime("%Y%m%d")
    
    st.info(f"ğŸ“¥ æé€ŸåŠ è½½æ•°æ®: {buffer_start} ~ {end_date} ...")
    
    # 2. æ‰¹é‡æ‹‰å–æ•°æ®
    chunk_dates = st.session_state.pro.trade_cal(start_date=buffer_start, end_date=end_date, is_open='1')['cal_date'].tolist()
    
    dfs = []
    # ç®€åŒ–è¿›åº¦æ˜¾ç¤º
    bar = st.progress(0)
    total_chunks = len(chunk_dates)
    
    for i, d in enumerate(chunk_dates):
        try:
            # åªæ‹‰å–å¿…è¦å­—æ®µ
            daily = st.session_state.pro.daily(trade_date=d, fields='ts_code,trade_date,open,high,low,close,pre_close,vol')
            if not daily.empty:
                # å‹ç¼©æ•°æ®ç±»å‹ float32 èŠ‚çœå†…å­˜
                for c in ['open','high','low','close','pre_close','vol']:
                    daily[c] = pd.to_numeric(daily[c], errors='coerce').astype('float32')
                dfs.append(daily)
        except: pass
        if i % 10 == 0: bar.progress((i+1)/total_chunks)
    bar.empty()
    
    if not dfs: return None
    
    # åˆå¹¶å¤§è¡¨
    df_all = pd.concat(dfs).sort_values(['ts_code', 'trade_date']).reset_index(drop=True)
    
    # 3. [å¤æƒå¤„ç†] 
    st.caption("ğŸ”§ æ­£åœ¨è¿›è¡Œå‘é‡åŒ–å¤æƒä¸æŒ‡æ ‡è®¡ç®—...")
    
    adj_dfs = []
    for i, d in enumerate(chunk_dates):
         try:
            adj = st.session_state.pro.adj_factor(trade_date=d, fields='ts_code,adj_factor')
            if not adj.empty:
                adj['trade_date'] = d
                adj_dfs.append(adj)
         except: pass
    
    if adj_dfs:
        adj_all = pd.concat(adj_dfs)
        df_all = pd.merge(df_all, adj_all, on=['ts_code', 'trade_date'], how='left')
        df_all['adj_factor'] = df_all['adj_factor'].fillna(method='ffill') 
        
        # è®¡ç®—åå¤æƒä»·æ ¼ç”¨äºæŒ‡æ ‡è®¡ç®—
        df_all['hfq_close'] = df_all['close'] * df_all['adj_factor']
    else:
        df_all['hfq_close'] = df_all['close']
    
    # 4. [å‘é‡åŒ–æŒ‡æ ‡è®¡ç®—]
    grouped = df_all.groupby('ts_code')['hfq_close']
    
    # MACD (8, 17, 5)
    ema8 = grouped.ewm(span=8, adjust=False).mean().reset_index(level=0, drop=True)
    ema17 = grouped.ewm(span=17, adjust=False).mean().reset_index(level=0, drop=True)
    
    df_all['diff'] = ema8 - ema17
    df_all['dea'] = df_all.groupby('ts_code')['diff'].ewm(span=5, adjust=False).mean().reset_index(level=0, drop=True)
    df_all['macd'] = (df_all['diff'] - df_all['dea']) * 2
    
    # MA20
    df_all['ma20'] = df_all.groupby('ts_code')['hfq_close'].rolling(20).mean().reset_index(level=0, drop=True)
    
    # MA5_Vol
    df_all['ma5_vol'] = df_all.groupby('ts_code')['vol'].rolling(5).mean().reset_index(level=0, drop=True)
    
    # 5. è¿‡æ»¤æ‰ç¼“å†²æœŸæ•°æ®ï¼Œåªä¿ç•™ç›®æ ‡å›æµ‹æœŸ
    df_target = df_all[df_all['trade_date'].isin(date_list)].copy()
    
    # ç«‹å³æ¸…ç†å†…å­˜
    del df_all, ema8, ema17, adj_dfs
    gc.collect()
    
    return df_target

def check_profit_with_stop_loss(ts_code, buy_date, buy_price, stop_loss_pct):
    """
    [é£æ§å¼•æ“] è·å–æœªæ¥æ”¶ç›Šï¼ŒåŒ…å«åˆšæ€§æ­¢æŸé€»è¾‘
    """
    try:
        d0 = datetime.strptime(buy_date, "%Y%m%d")
        f_start = (d0 + timedelta(days=1)).strftime("%Y%m%d")
        f_end = (d0 + timedelta(days=15)).strftime("%Y%m%d")
        
        # æ‹‰å–å•åªè‚¡ç¥¨æœªæ¥æ•°æ®
        df = st.session_state.pro.daily(ts_code=ts_code, start_date=f_start, end_date=f_end, fields='trade_date,open,high,low,close,pre_close')
        if df.empty: return {}
        
        df = df.sort_values('trade_date').reset_index(drop=True)
        
        # æ£€æŸ¥ä¹°å…¥æ¡ä»¶ (T+1)
        d1 = df.iloc[0]
        limit_up = d1['pre_close'] * 1.095
        if d1['open'] <= d1['pre_close']: return {'status': 'ä½å¼€æ”¾å¼ƒ'}
        if d1['open'] >= limit_up and d1['low'] >= d1['open']: return {'status': 'ä¸€å­—æ¿æ”¾å¼ƒ'}
        if d1['high'] < buy_price: return {'status': 'æœªçªç ´'}
        
        # æˆäº¤
        res = {'status': 'æˆäº¤'}
        stop_price = buy_price * (1 + stop_loss_pct/100)
        
        for n in [1, 3, 5]:
            if len(df) >= n:
                triggered_stop = False
                for i in range(n):
                    day_low = df.iloc[i]['low']
                    if day_low <= stop_price:
                        # è§¦å‘æ­¢æŸï¼ŒæŒ‰æ­¢æŸä»·æˆ–å¼€ç›˜ä»·ç¦»åœº
                        exit_price = min(stop_price, df.iloc[i]['open'])
                        ret = (exit_price / buy_price - 1) * 100
                        res[f'Return_D{n} (%)'] = ret
                        res[f'Stop_D{n}'] = True
                        triggered_stop = True
                        break 
                
                if not triggered_stop:
                    close_price = df.iloc[n-1]['close']
                    res[f'Return_D{n} (%)'] = (close_price / buy_price - 1) * 100
                    res[f'Stop_D{n}'] = False
                    
        return res
        
    except: return {}

# ---------------------------
# ä¸»ç¨‹åº
# ---------------------------
if start_btn:
    if not st.session_state.ts_token:
        st.error("âŒ è¯·å…ˆè¾“å…¥ Token")
        st.stop()
        
    # 1. è·å–æ—¥æœŸ
    end_date_str = datetime.now().strftime("%Y%m%d")
    all_days = get_trade_days(end_date_str, backtest_days)
    all_days = sorted(all_days)
    
    # 2. æ™ºèƒ½åˆ†æ®µ
    # [å…³é”®ä¿®æ”¹] è¿™é‡Œè®¾ç½®ä¸º 15ï¼Œè§£å†³ 500 å¤©å›æµ‹å¡æ­»é—®é¢˜
    BATCH_SIZE = 15
    batches = [all_days[i:i + BATCH_SIZE] for i in range(0, len(all_days), BATCH_SIZE)]
    
    results_file = "rank_free_v3_results.csv"
    if os.path.exists(results_file): os.remove(results_file) 
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    total_trades = 0
    
    for b_i, batch_days in enumerate(batches):
        status_text.markdown(f"### âš¡ æ­£åœ¨æé€Ÿè®¡ç®—: {batch_days[0]} ~ {batch_days[-1]} ({b_i+1}/{len(batches)})")
        
        # A. å‘é‡åŒ–å‡†å¤‡æ•°æ®
        df_batch = load_data_and_compute_vectorized(batch_days)
        if df_batch is None or df_batch.empty: continue
        
        # B. æ¯æ—¥é€‰è‚¡
        batch_results = []
        
        for day in batch_days:
            try:
                # å†…å­˜ç­›é€‰
                day_data = df_batch[df_batch['trade_date'] == day]
                if day_data.empty: continue
                
                # è·å– Basic (æ¢æ‰‹ã€é‡æ¯”ã€å¸‚å€¼)
                basic = st.session_state.pro.daily_basic(trade_date=day, fields='ts_code,turnover_rate,volume_ratio,circ_mv')
                if basic is None or basic.empty: continue
                
                # åˆå¹¶
                merged = pd.merge(day_data, basic, on='ts_code', how='inner')
                
                # V30.22 æ ¸å¿ƒç­›é€‰
                mask = (
                    (merged['hfq_close'] > merged['ma20']) &
                    (merged['vol'] > merged['ma5_vol'] * 1.2) &
                    (merged['macd'] > 0) &
                    (merged['close'] >= min_price) & 
                    (merged['close'] <= max_price) &
                    (merged['turnover_rate'] > 3.0) &
                    (merged['circ_mv'] > 200000) 
                )
                candidates = merged[mask].copy()
                
                if candidates.empty: continue
                
                # è¯„åˆ†
                candidates['base_score'] = (candidates['macd'] / candidates['hfq_close']) * 1000000
                
                candidates['pct_chg'] = (candidates['close'] / candidates['pre_close'] - 1) * 100
                candidates['bonus'] = 1.0
                candidates.loc[(candidates['volume_ratio'] > 1.5) & (candidates['volume_ratio'] < 5.0), 'bonus'] += 0.1
                candidates.loc[(candidates['turnover_rate'] > 5.0) & (candidates['turnover_rate'] < 15.0), 'bonus'] += 0.1
                candidates.loc[candidates['pct_chg'] > 9.5, 'bonus'] += 0.1
                
                candidates['final_score'] = candidates['base_score'] * candidates['bonus']
                
                # [å…³é”®ä¿®æ”¹] ä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰çš„ top_k
                top_selection = candidates.sort_values('final_score', ascending=False).head(top_k)
                
                # æ¨¡æ‹Ÿäº¤æ˜“
                for row in top_selection.itertuples():
                    buy_price = row.open * (1 + buy_threshold/100)
                    res = check_profit_with_stop_loss(row.ts_code, day, buy_price, stop_loss_pct)
                    
                    if res.get('status') == 'æˆäº¤':
                        rec = {
                            'trade_date': day,
                            'ts_code': row.ts_code,
                            'close': row.close,
                            'score': row.final_score
                        }
                        rec.update(res)
                        batch_results.append(rec)
            
            except Exception: pass
        
        # C. å†™å…¥ç»“æœ
        if batch_results:
            df_res = pd.DataFrame(batch_results)
            df_res.to_csv(results_file, mode='a', header=not os.path.exists(results_file), index=False, encoding='utf-8-sig')
            total_trades += len(df_res)
            st.toast(f"âœ… æ–°å¢ {len(df_res)} ç¬”äº¤æ˜“ (ç´¯è®¡: {total_trades})")
            
        progress_bar.progress((b_i + 1) / len(batches))
        gc.collect() # å¼ºåˆ¶åƒåœ¾å›æ”¶

    st.success("ğŸ‰ æé€Ÿå›æµ‹å®Œæˆï¼")
    
    # ç»“æœå±•ç¤º
    if os.path.exists(results_file):
        res_df = pd.read_csv(results_file)
        st.subheader("ğŸ“Š æœ€ç»ˆå›æµ‹æŠ¥å‘Š")
        
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("æ€»äº¤æ˜“æ¬¡æ•°", len(res_df))
        
        if 'Return_D1 (%)' in res_df.columns:
            avg_d1 = res_df['Return_D1 (%)'].mean()
            win_d1 = (res_df['Return_D1 (%)'] > 0).mean() * 100
            c2.metric("D+1 å‡æ”¶", f"{avg_d1:.2f}%")
            c3.metric("D+1 èƒœç‡", f"{win_d1:.1f}%")
            
            # ç®€å•å›æ’¤è®¡ç®—
            res_df = res_df.sort_values('trade_date')
            # å‡è®¾æ¯æ—¥å‡ä»“
            daily_ret = res_df.groupby('trade_date')['Return_D1 (%)'].mean()
            equity = daily_ret.cumsum()
            dd = equity.cummax() - equity
            c4.metric("æœ€å¤§å›æ’¤", f"{dd.max():.2f}")
            
        st.dataframe(res_df, use_container_width=True)
        with open(results_file, "rb") as f:
            st.download_button("ğŸ“¥ ä¸‹è½½è¯¦ç»†æˆ˜æŠ¥", f, "rank_free_v3_fast.csv")
