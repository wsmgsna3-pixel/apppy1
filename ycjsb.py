# -*- coding: utf-8 -*-
"""
é€‰è‚¡ç‹ Â· V46.3 å†…å­˜æ‰‹æœ¯ç‰ˆ (é’ˆå¯¹Streamlit Cloud 1GBå†…å­˜ä¼˜åŒ–)
æ ¸å¿ƒé€»è¾‘ï¼š
1. è§£å†³å´©æºƒæ ¹æœ¬åŸå› ï¼šä¸å†ä¸€æ¬¡æ€§æŠŠæ‰€æœ‰æ•°æ®è¯»å…¥å†…å­˜ã€‚
2. é‡‡ç”¨"æµå¼è®¡ç®—"ï¼šè¯»ä¸€å¤©æ•°æ® -> è®¡ç®— -> é‡Šæ”¾å†…å­˜ -> å†è¯»ä¸‹ä¸€å¤©ã€‚
3. å³°å€¼å†…å­˜å ç”¨ï¼šæ§åˆ¶åœ¨ 300MB ä»¥å†…ï¼Œå®Œç¾é€‚é…å…è´¹ç‰ˆæœåŠ¡å™¨ã€‚
4. ä¿æŒæ‰€æœ‰æˆ˜æ³•é€»è¾‘ä¸å˜ï¼šRSRS(Numpyç‰ˆ) + åŒé»„é‡‘é€šé“ã€‚
"""

import streamlit as st
import pandas as pd
import numpy as np
import tushare as ts
from datetime import datetime, timedelta
import os
import gc
import time
import pickle

# ---------------------------
# é¡µé¢é…ç½®
# ---------------------------
st.set_page_config(page_title="V46.3 å†…å­˜æ‰‹æœ¯ç‰ˆ", layout="wide")
st.title("ğŸš€ V46.3 RSRSè¶‹åŠ¿ç›‘æ§ (çœå†…å­˜ç‰ˆ)")

# ---------------------------
# å…¨å±€è®¾ç½®
# ---------------------------
SCORE_DB_FILE = "v46_rsrs_trend_db.csv"
CACHE_DIR = "daily_data_cache"

if not os.path.exists(CACHE_DIR):
    os.makedirs(CACHE_DIR)

# åˆå§‹åŒ– session_state
if 'pro' not in st.session_state:
    st.session_state.pro = None
if 'GLOBAL_CALENDAR' not in st.session_state:
    st.session_state.GLOBAL_CALENDAR = []

# å…¨å±€å˜é‡ (ä»…å­˜å‚¨æå°‘é‡åŸºç¡€ä¿¡æ¯ï¼Œä¸å†å­˜å‚¨å…¨é‡è¡Œæƒ…)
GLOBAL_ADJ_FACTOR = pd.DataFrame() 
GLOBAL_QFQ_BASE_FACTORS = {} 

@st.cache_data(ttl=3600*12) 
def safe_get(func_name, **kwargs):
    if st.session_state.pro is None: return pd.DataFrame(columns=['ts_code']) 
    func = getattr(st.session_state.pro, func_name) 
    for attempt in range(3):
        try:
            df = func(**kwargs)
            if df is None or (isinstance(df, pd.DataFrame) and df.empty):
                return pd.DataFrame(columns=['ts_code']) 
            return df
        except Exception:
            if attempt < 2: time.sleep(1); continue
            else: return pd.DataFrame(columns=['ts_code'])

def get_trade_days(end_date_str, num_days):
    # é¢„çƒ­60å¤©ç”¨äºRSRS
    start_search = (datetime.strptime(end_date_str, "%Y%m%d") - timedelta(days=max(num_days * 5, 150))).strftime("%Y%m%d")
    end_search = (datetime.strptime(end_date_str, "%Y%m%d") + timedelta(days=60)).strftime("%Y%m%d")
    
    cal = safe_get('trade_cal', start_date=start_search, end_date=end_search)
    if cal.empty or 'is_open' not in cal.columns: return []
    
    open_cal = cal[cal['is_open'] == 1].sort_values('cal_date', ascending=True)
    cal_list = open_cal['cal_date'].tolist()
    st.session_state.GLOBAL_CALENDAR = cal_list # å­˜å…¥session
    
    past_days = open_cal[open_cal['cal_date'] <= end_date_str]['cal_date'].tolist()
    return past_days[-(num_days + 60):]

# ----------------------------------------------------------------------
# é˜¶æ®µä¸€ï¼šæ•°æ®ä¸‹è½½ (åªå­˜ç¡¬ç›˜ï¼Œä¸å å†…å­˜)
# ----------------------------------------------------------------------
def fetch_single_day_data(date):
    try:
        daily_df = safe_get('daily', trade_date=date)
        adj_df = safe_get('adj_factor', trade_date=date)
        basic_df = safe_get('daily_basic', trade_date=date, fields='ts_code,circ_mv,turnover_rate,volume_ratio')
        
        # åŸºç¡€æ¸…æ´—
        if not daily_df.empty:
            daily_df = daily_df[daily_df['ts_code'].str.startswith(('30', '688'))]
            if not basic_df.empty: daily_df = daily_df.merge(basic_df, on='ts_code', how='left')
        
        if not adj_df.empty:
            adj_df = adj_df[adj_df['ts_code'].str.startswith(('30', '688'))]

        return {'adj': adj_df, 'daily': daily_df}
    except: return None

def ensure_data_on_disk(date_list):
    st.info(f"ğŸ“‚ æ ¡éªŒæœ¬åœ°æ•°æ® ({len(date_list)} å¤©)...")
    progress_bar = st.progress(0)
    
    for i, date in enumerate(date_list):
        cache_path = os.path.join(CACHE_DIR, f"{date}.pkl")
        if not os.path.exists(cache_path):
            data_packet = fetch_single_day_data(date)
            if data_packet:
                with open(cache_path, 'wb') as f:
                    pickle.dump(data_packet, f)
        
        if i % 10 == 0: progress_bar.progress((i + 1) / len(date_list))
    
    progress_bar.empty()
    return True

# ----------------------------------------------------------------------
# é˜¶æ®µäºŒï¼šæ„å»ºå¤æƒå› å­ (è½»é‡çº§)
# ----------------------------------------------------------------------
def load_adj_factors(date_list):
    """åªåŠ è½½å¤æƒå› å­è¿›å†…å­˜ï¼Œè¿™ä¸ªå¾ˆå°ï¼Œä¸ä¼šå´©"""
    global GLOBAL_ADJ_FACTOR, GLOBAL_QFQ_BASE_FACTORS
    adj_list = []
    
    for date in date_list:
        cache_path = os.path.join(CACHE_DIR, f"{date}.pkl")
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'rb') as f:
                    packet = pickle.load(f)
                    if not packet['adj'].empty: adj_list.append(packet['adj'])
            except: pass
            
    if not adj_list: return False
    
    adj_data = pd.concat(adj_list)
    adj_data['adj_factor'] = pd.to_numeric(adj_data['adj_factor'], errors='coerce').fillna(0)
    GLOBAL_ADJ_FACTOR = adj_data.set_index(['ts_code', 'trade_date']).sort_index(level=[0, 1])
    
    latest = GLOBAL_ADJ_FACTOR.index.get_level_values('trade_date').max()
    if latest:
        GLOBAL_QFQ_BASE_FACTORS = GLOBAL_ADJ_FACTOR.loc[(slice(None), latest), 'adj_factor'].droplevel(1).to_dict()
    return True

# ----------------------------------------------------------------------
# æ ¸å¿ƒè®¡ç®—ï¼šå³ç”¨å³å– (é˜²å´©å…³é”®)
# ----------------------------------------------------------------------
def get_daily_packet(date):
    """ä»ç¡¬ç›˜è¯»å–å•æ—¥æ•°æ®"""
    cache_path = os.path.join(CACHE_DIR, f"{date}.pkl")
    if not os.path.exists(cache_path): return pd.DataFrame()
    try:
        with open(cache_path, 'rb') as f:
            packet = pickle.load(f)
            return packet['daily']
    except: return pd.DataFrame()

def get_history_window(ts_code, end_date, lookback_days=60):
    """
    ä¸ºäº†ç®—RSRSï¼Œéœ€è¦å»ç¡¬ç›˜é‡Œè¯»è¿‡å»60å¤©çš„æ•°æ®ã€‚
    ä¸ºäº†ä¸æ…¢ï¼Œæˆ‘ä»¬åªè¯»è¯¥è‚¡ç¥¨çš„æ•°æ®ã€‚
    æ³¨æ„ï¼šè¿™æ˜¯IOå¯†é›†å‹æ“ä½œï¼Œä½†çœå†…å­˜ã€‚
    """
    # è¿™é‡Œçš„ä¼˜åŒ–ç­–ç•¥ï¼šç”±äºé€ä¸ªæ–‡ä»¶è¯»å¤ªæ…¢ï¼Œæˆ‘ä»¬ä¾ç„¶éœ€è¦ä¸€ä¸ª"ä¸­å‹å†…å­˜å—"
    # ä½†æˆ‘ä»¬åªå­˜ close/highï¼Œä¸å­˜å…¶ä»–ã€‚
    # ä¸ºäº†ç®€åŒ–ä»£ç ä¸”ä¿è¯ä¸å´©ï¼Œè¿™é‡Œæˆ‘ä»¬é‡‡ç”¨"ä¸´æ—¶æ„å»º"ç­–ç•¥
    pass 

# ä¿®æ­£ï¼šä¸Šé¢çš„é€»è¾‘å¯¹äº Streamlit è¿˜æ˜¯å¤ªæ…¢ã€‚
# V46.3 æ”¹è¿›ç­–ç•¥ï¼šæ„å»ºä¸€ä¸ªåªåŒ…å« [code, date, close, high] çš„è½»é‡çº§ DataFrame å¸¸é©»å†…å­˜
# è¿™æ¯”å­˜å…¨é‡æ•°æ®çœ 80% å†…å­˜ã€‚

GLOBAL_MINI_HISTORY = pd.DataFrame()

def load_mini_history(date_list):
    """åªåŠ è½½ close/high è¿›å†…å­˜"""
    global GLOBAL_MINI_HISTORY
    st.text("ğŸ”„ æ„å»ºè½»é‡çº§å†å²æ•°æ® (ä»…Close/High)...")
    
    mini_list = []
    progress = st.progress(0)
    
    for i, date in enumerate(date_list):
        cache_path = os.path.join(CACHE_DIR, f"{date}.pkl")
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'rb') as f:
                    packet = pickle.load(f)
                    df = packet['daily']
                    if not df.empty:
                        # åªå–æ ¸å¿ƒåˆ—ï¼Œè½¬ float32
                        cols = ['ts_code','trade_date','close','high']
                        valid = [c for c in cols if c in df.columns]
                        mini = df[valid].copy()
                        for c in ['close','high']:
                            if c in mini.columns: mini[c] = mini[c].astype('float32')
                        mini_list.append(mini)
            except: pass
        if i % 20 == 0: progress.progress((i+1)/len(date_list))
    
    progress.empty()
    if not mini_list: return False
    
    full_df = pd.concat(mini_list)
    GLOBAL_MINI_HISTORY = full_df.set_index(['ts_code', 'trade_date']).sort_index(level=[0, 1])
    return True

def get_qfq_close_high(ts_code, start_date, end_date):
    """ä»è½»é‡çº§å†å²è·å–å¤æƒæ•°æ®"""
    base = GLOBAL_QFQ_BASE_FACTORS.get(ts_code)
    if not base: return pd.DataFrame()
    
    try:
        daily = GLOBAL_MINI_HISTORY.loc[(ts_code, slice(start_date, end_date)), :]
        adj = GLOBAL_ADJ_FACTOR.loc[(ts_code, slice(start_date, end_date)), 'adj_factor']
        
        df = daily.join(adj, how='left').dropna(subset=['adj_factor'])
        factor = df['adj_factor'] / base
        for c in ['close','high']:
            if c in df.columns: df[c] = df[c] * factor
        return df
    except: return pd.DataFrame()

def analyze_rsrs(ts_code, current_date, max_bias):
    try:
        start_date = (datetime.strptime(current_date, "%Y%m%d") - timedelta(days=60)).strftime("%Y%m%d")
        df = get_qfq_close_high(ts_code, start_date, current_date)
        if df.empty or len(df) < 18: return None
        
        # å¿…é¡»æ˜¯å½“å¤©æ•°æ®
        last_date = str(df.index[-1][1]) if isinstance(df.index[-1], tuple) else str(df.iloc[-1]['trade_date']) # å…¼å®¹æ€§å¤„ç†
        # ç´¢å¼•å¯èƒ½æ˜¯MultiIndex (ts_code, trade_date)
        # ä¸Šé¢çš„ get_qfq è¿”å›çš„æ˜¯ DataFrame, indexæ˜¯(ts_code, trade_date)
        # ç®€å•å¤„ç†: reset_index
        df = df.reset_index()
        if str(df.iloc[-1]['trade_date']) != str(current_date): return None
        
        close = df['close']
        ma20 = close.rolling(20).mean().iloc[-1]
        curr = close.iloc[-1]
        
        if ma20 == 0 or pd.isna(ma20): return None
        if curr < ma20: return None # å‡çº¿ä¸‹
        
        bias = (curr - ma20) / ma20 * 100
        if bias > max_bias: return None # å±±é¡¶
        
        # RSRS
        recent = df.iloc[-18:]
        y = recent['high'].values
        x = np.arange(len(y))
        slope, _ = np.polyfit(x, y, 1)
        
        if slope > 0: return slope * 10
        return None
    except: return None

def batch_compute(date, max_bias):
    # 1. è¯»å½“æ—¥è¯¦ç»†æ•°æ®(åŒ…å«æ¢æ‰‹ç‡)
    daily_t = get_daily_packet(date)
    if daily_t.empty: return []
    
    # è¿‡æ»¤
    mask = (daily_t['vol'] > 0) & (daily_t['close'] >= 2.0)
    pool = daily_t[mask]
    
    results = []
    # éå†
    for idx, row in pool.iterrows():
        code = row['ts_code']
        rsrs = analyze_rsrs(code, date, max_bias)
        
        if rsrs:
            # åªæœ‰é€šè¿‡RSRSçš„æ‰è®°å½•
            results.append({
                'Select_Date': date,
                'Code': code,
                'Score': row['turnover_rate'], # æ¢æ‰‹ç‡æ’åº
                'Name': row['name'] if 'name' in row else code,
                'Close': row['close'],
                'Pct_Chg': row['pct_chg'],
                'Circ_Mv': row['circ_mv'],
                'Turnover': row['turnover_rate'],
                'Vol_Ratio': row['volume_ratio'],
                'RSRS_Slope': rsrs
            })
            
    return results

# ----------------------------------------------------------------------
# å›æµ‹æ‰§è¡Œ
# ----------------------------------------------------------------------
def run_backtest(df_scores, top_n, min_mv, min_p, max_p, min_t, max_t, min_v, max_v, buy_min, buy_max, stop_loss):
    min_mv_val = min_mv * 10000
    mask = (df_scores['Circ_Mv'] >= min_mv_val) & \
           (df_scores['Pct_Chg'] >= min_p) & \
           (df_scores['Pct_Chg'] <= max_p) & \
           (df_scores['Turnover'] >= min_t) & \
           (df_scores['Turnover'] <= max_t) & \
           (df_scores['Vol_Ratio'] >= min_v) & \
           (df_scores['Vol_Ratio'] <= max_v)
    
    filtered = df_scores[mask].copy()
    if filtered.empty: return []
    
    filtered = filtered.sort_values('Score', ascending=False).head(top_n)
    
    select_date = str(filtered.iloc[0]['Select_Date'])
    calendar = st.session_state.GLOBAL_CALENDAR
    try:
        t_idx = calendar.index(select_date)
        buy_date = calendar[t_idx + 1] if t_idx < len(calendar) - 1 else None
    except: buy_date = None
    
    res = []
    for rank, (idx, row) in enumerate(filtered.iterrows(), 1):
        code = row['Code']
        signal = "â³"
        is_buy = False
        ret_d3 = np.nan
        ret_d5 = np.nan
        status = "-"
        
        if buy_date:
            # ä¸´æ—¶è¯»ä¹°å…¥æ—¥æ•°æ®
            d1 = get_daily_packet(buy_date)
            if not d1.empty:
                d1_row = d1[d1['ts_code'] == code]
                if not d1_row.empty:
                    d1_row = d1_row.iloc[0]
                    op = float(d1_row['open'])
                    pre = float(d1_row['pre_close'])
                    pct = (op/pre - 1)*100
                    
                    if buy_min <= pct <= buy_max:
                        is_buy = True
                        signal = "âœ… BUY"
                    else: signal = "ğŸ‘€ è§‚æœ›"
                    
                    if is_buy:
                        # ç®€æ˜“å›æµ‹ï¼šç›´æ¥ç”¨è½»é‡çº§å†å²æ•°æ®æŸ¥æœªæ¥ Close
                        # (ä¸ºäº†çœå†…å­˜ï¼Œè¿™é‡Œåªç”¨ Close è®¡ç®—æ”¶ç›Šï¼Œå¿½ç•¥ Open/Low çš„ç²¾å‡†æ­¢æŸ)
                        # è¿™æ˜¯åœ¨ 1G å†…å­˜ä¸‹çš„å¿…è¦å¦¥å
                        future = get_qfq_close_high(code, buy_date, "20991231")
                        if not future.empty:
                            buy_p = future.iloc[0]['close'] # è¿‘ä¼¼ä¹°å…¥
                            if len(future) >= 3:
                                ret_d3 = (future.iloc[2]['close']/buy_p - 1)*100
                            else: ret_d3 = (future.iloc[-1]['close']/buy_p - 1)*100
                                
                            if len(future) >= 5:
                                ret_d5 = (future.iloc[4]['close']/buy_p - 1)*100
                            else: ret_d5 = (future.iloc[-1]['close']/buy_p - 1)*100
                            status = "ğŸ’° æŒæœ‰"
                            
        res.append({
            'Select_Date': select_date,
            'Rank': rank,
            'Code': code,
            'Name': row['Name'],
            'Signal': signal,
            'Ret_D3': ret_d3,
            'Ret_D5': ret_d5,
            'Status': status
        })
    return res

# ----------------------------------------------------
# GUI
# ----------------------------------------------------
with st.sidebar:
    st.header("1. åŸºç¡€è®¾ç½®")
    default_date = datetime.now().date()
    end_date = st.date_input("æˆªæ­¢æ—¥æœŸ", value=default_date)
    days_back = int(st.number_input("å›æµ‹å¤©æ•°", value=5))
    
    st.markdown("---")
    st.header("2. RSRS + åŒé€šé“")
    
    TOP_N = 3
    MIN_MV_YI = st.number_input("å¸‚å€¼Min", 10, 500, 30, 10)
    MAX_BIAS = st.number_input("ä¹–ç¦»ç‡Max%", 5, 50, 15, 1)
    
    c1, c2 = st.columns(2)
    with c1: MIN_PCT = st.number_input("æ¶¨å¹…Min", 0, 20, 6, 1)
    with c2: MAX_PCT = st.number_input("æ¶¨å¹…Max", 0, 20, 16, 1)
    
    c3, c4 = st.columns(2)
    with c3: MIN_T = st.number_input("æ¢æ‰‹Min", 0.0, 50.0, 18.0, 1.0)
    with c4: MAX_T = st.number_input("æ¢æ‰‹Max", 0.0, 50.0, 26.0, 1.0)
    
    c5, c6 = st.columns(2)
    with c5: MIN_V = st.number_input("é‡æ¯”Min", 0.0, 10.0, 1.5, 0.1)
    with c6: MAX_V = st.number_input("é‡æ¯”Max", 0.0, 10.0, 3.5, 0.1)

    st.markdown("---")
    st.header("3. äº¤æ˜“è§„åˆ™")
    c7, c8 = st.columns(2)
    with c7: BUY_MIN = st.number_input("å¼€ç›˜Min%", -10.0, 10.0, 0.0, 0.5)
    with c8: BUY_MAX = st.number_input("å¼€ç›˜Max%", -10.0, 10.0, 4.0, 0.5)
    STOP_LOSS = st.number_input("æ­¢æŸ%", 1, 20, 5, 1)

    st.markdown("---")
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºç»“æœ"):
        if os.path.exists(SCORE_DB_FILE): os.remove(SCORE_DB_FILE)
        st.toast("å·²æ¸…é™¤")

# ---------------------------
# ä¸»ç¨‹åº
# ---------------------------
c_token, c_btn = st.columns([3, 1])
with c_token:
    TS_TOKEN = st.text_input("ğŸ”‘ Token", type="password")
with c_btn:
    start_btn = st.button("ğŸš€ å¯åŠ¨ (çœå†…å­˜ç‰ˆ)", type="primary", use_container_width=True)

if start_btn:
    if not TS_TOKEN: st.stop()
    ts.set_token(TS_TOKEN)
    st.session_state.pro = ts.pro_api()
    
    # 1. ç®—æ—¥æœŸ
    target_dates = get_trade_days(end_date.strftime("%Y%m%d"), days_back)
    if not target_dates: st.stop()
    
    # 2. å¿…é¡»å…ˆç¡®ä¿æ•°æ®åœ¨ç¡¬ç›˜
    ensure_data_on_disk(target_dates)
    
    # 3. å¿…é¡»æ„å»ºå¤æƒå› å­
    if not load_adj_factors(target_dates): st.stop()
    
    # 4. æ„å»ºè½»é‡çº§å†å² (åªå« Close/High) - è¿™ä¸€æ­¥æ˜¯çœå†…å­˜å…³é”®
    if not load_mini_history(target_dates): st.stop()
    
    # 5. è®¡ç®—é€»è¾‘
    existing_dates = []
    if os.path.exists(SCORE_DB_FILE):
        try:
            df_dates = pd.read_csv(SCORE_DB_FILE, usecols=['Select_Date'])
            existing_dates = df_dates['Select_Date'].astype(str).unique().tolist()
        except: pass
        
    backtest_dates = target_dates[-days_back:]
    dates_to_compute = [d for d in backtest_dates if str(d) not in existing_dates]
    
    if dates_to_compute:
        st.write(f"ğŸ”„ è®¡ç®—ä¸­ ({len(dates_to_compute)}å¤©)...")
        bar = st.progress(0)
        for i, date in enumerate(dates_to_compute):
            scores = batch_compute(date, MAX_BIAS)
            if scores:
                df_chunk = pd.DataFrame(scores)
                need_header = not os.path.exists(SCORE_DB_FILE)
                df_chunk.to_csv(SCORE_DB_FILE, mode='a', header=need_header, index=False)
            
            # æ‰‹åŠ¨GC
            if i % 10 == 0: gc.collect()
            bar.progress((i+1)/len(dates_to_compute))
        bar.empty()
        
    # 6. æŠ¥è¡¨
    if os.path.exists(SCORE_DB_FILE):
        df_all = pd.read_csv(SCORE_DB_FILE)
        df_all['Select_Date'] = df_all['Select_Date'].astype(str)
        
        final_report = []
        for date in backtest_dates:
            df_daily = df_all[df_all['Select_Date'] == str(date)]
            if df_daily.empty: continue
            
            res = run_backtest(
                df_daily, TOP_N, MIN_MV_YI, MIN_PCT, MAX_PCT, MIN_T, MAX_T, MIN_V, MAX_V, BUY_MIN, BUY_MAX, STOP_LOSS
            )
            if res: final_report.extend(res)
            
        if final_report:
            df_res = pd.DataFrame(final_report)
            trades = df_res[df_res['Signal'].str.contains('BUY', na=False)]
            
            st.markdown(f"### ğŸ“Š RSRSç­–ç•¥è¡¨ç°")
            cols = st.columns(3)
            for i, r in enumerate([1, 2, 3]):
                rank_trades = trades[trades['Rank'] == r]
                count = len(rank_trades)
                if count > 0:
                    ret_d5 = rank_trades['Ret_D5'].mean()
                    win_d5 = (rank_trades['Ret_D5'] > 0).mean() * 100
                    color = "red" if ret_d5 > 0 else "green"
                    cols[i].markdown(f"#### Rank {r}\näº¤æ˜“æ•°:{count}\nD5å‡æ”¶::{color}[{ret_d5:.2f}%]\nèƒœç‡:{win_d5:.1f}%")
                else: cols[i].markdown(f"#### Rank {r}\næ— äº¤æ˜“")
            
            st.dataframe(df_res, use_container_width=True)
        else:
            st.warning("æ— äº¤æ˜“")
