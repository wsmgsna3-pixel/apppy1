import streamlit as st
import pandas as pd
import numpy as np
import tushare as ts
from datetime import datetime, timedelta
import warnings
import time  
warnings.filterwarnings("ignore")

# ---------------------------
# å…¨å±€å˜é‡åˆå§‹åŒ–
# ---------------------------
pro = None 
GLOBAL_ADJ_FACTOR = pd.DataFrame() 
GLOBAL_DAILY_RAW = pd.DataFrame() 
GLOBAL_QFQ_BASE_FACTORS = {} # {ts_code: latest_adj_factor}


# ---------------------------
# é¡µé¢è®¾ç½®
# ---------------------------
st.set_page_config(page_title="é€‰è‚¡ç‹ V30.11 Alpha æ¢å¤ç‰ˆ", layout="wide")
st.title("é€‰è‚¡ç‹ V30.11ï¼šAlpha æ¢å¤ç‰ˆï¼ˆâœ… ä¿®å¤ NameErrorï¼‰")
st.markdown("ğŸ¯ **V30.11 ç­–ç•¥æ ¸å¿ƒï¼š** æ ¸å¿ƒé€»è¾‘ä¸ V30.10 ä¿æŒä¸€è‡´ï¼Œå…¨åŠ›æ¢å¤ V30.8 çš„è¶…é«˜ D+5 æ”¶ç›Šã€‚æœ¬ç‰ˆæœ¬ä»…ä¸ºä¿®å¤ä»£ç é”™è¯¯ã€‚")

# ---------------------------
# è¾…åŠ©å‡½æ•° (APIè°ƒç”¨å’Œæ•°æ®è·å–)
# ---------------------------
@st.cache_data(ttl=3600*12) 
def safe_get(func_name, **kwargs):
    """å®‰å…¨è°ƒç”¨ Tushare API"""
    global pro
    if pro is None:
        return pd.DataFrame(columns=['ts_code']) 
    func = getattr(pro, func_name) 
    try:
        # V30.0 æ–°å¢ï¼šæ”¯æŒæŒ‡æ•°æ¥å£ (åªæœ‰ daily æ¥å£æœ‰ index å‚æ•°)
        if kwargs.get('is_index'):
             df = pro.index_daily(**kwargs)
        else:
             df = func(**kwargs)

        if df is None or (isinstance(df, pd.DataFrame) and df.empty):
            return pd.DataFrame(columns=['ts_code']) 
        return df
    except Exception as e:
        return pd.DataFrame(columns=['ts_code'])

def get_trade_days(end_date_str, num_days):
    """è·å– num_days ä¸ªäº¤æ˜“æ—¥ä½œä¸ºé€‰è‚¡æ—¥"""
    start_date = (datetime.strptime(end_date_str, "%Y%m%d") - timedelta(days=num_days * 2)).strftime("%Y%m%d")
    cal = safe_get('trade_cal', start_date=start_date, end_date=end_date_str)
    
    if cal.empty or 'is_open' not in cal.columns:
        st.error("æ— æ³•è·å–äº¤æ˜“æ—¥å†ï¼Œè¯·æ£€æŸ¥ Token æˆ– Tushare æƒé™ã€‚")
        return []
    
    trade_days_df = cal[cal['is_open'] == 1].sort_values('cal_date', ascending=False)
    trade_days_df = trade_days_df[trade_days_df['cal_date'] <= end_date_str]
    return trade_days_df['cal_date'].head(num_days).tolist()


# ----------------------------------------------------------------------
# â­ï¸ V30.4.4 æ–°å¢ï¼šæŒ‰æ—¥ç¼“å­˜æ•°æ®å‡½æ•° (è§£å†³é•¿å›æµ‹ä¸­æ–­é—®é¢˜)
# ----------------------------------------------------------------------
@st.cache_data(ttl=3600*24)
def fetch_and_cache_daily_data(date):
    """å®‰å…¨æ‹‰å–å¹¶ç¼“å­˜å•ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®"""
    adj_df = safe_get('adj_factor', trade_date=date)
    daily_df = safe_get('daily', trade_date=date)
    
    # è¿”å›ä¸€ä¸ªåŒ…å«è¯¥æ—¥æœŸæ•°æ®çš„å­—å…¸ï¼Œä¾¿äºåç»­åˆå¹¶
    return {
        'adj': adj_df,
        'daily': daily_df,
    }

# ----------------------------------------------------------------------
# æ ¸å¿ƒåŠ é€Ÿå‡½æ•°ï¼šæŒ‰æ—¥æœŸå¾ªç¯æ‹‰å–å†å²æ•°æ® 
# ----------------------------------------------------------------------
def get_all_historical_data(trade_days_list):
    """
    é€šè¿‡å¾ªç¯è°ƒç”¨ fetch_and_cache_daily_data æ„å»ºå…¨å±€æ•°æ®ï¼Œ
    åˆ©ç”¨ Streamlit çš„ fine-grained ç¼“å­˜æœºåˆ¶é¿å…é‡å¤ä¸‹è½½ã€‚
    """
    global GLOBAL_ADJ_FACTOR, GLOBAL_DAILY_RAW, GLOBAL_QFQ_BASE_FACTORS
    
    if not trade_days_list: return False
    
    latest_trade_date = max(trade_days_list) 
    earliest_trade_date = min(trade_days_list)
    
    # æ‰©å¤§æ•°æ®è·å–èŒƒå›´ (150å¤©å†å² + 20å¤©æœªæ¥)
    start_date_dt = datetime.strptime(earliest_trade_date, "%Y%m%d") - timedelta(days=150)
    end_date_dt = datetime.strptime(latest_trade_date, "%Y%m%d") + timedelta(days=20)
    
    start_date = start_date_dt.strftime("%Y%m%d")
    end_date = end_date_dt.strftime("%Y%m%d")
    
    # 1. è·å–æ‰€æœ‰äº¤æ˜“æ—¥åˆ—è¡¨
    all_trade_dates_df = safe_get('trade_cal', start_date=start_date, end_date=end_date, is_open='1')
    if all_trade_dates_df.empty:
        st.error("æ— æ³•è·å–äº¤æ˜“æ—¥å†ã€‚")
        return False
    
    all_dates = all_trade_dates_df['cal_date'].tolist()
    st.info(f"â³ æ­£åœ¨æŒ‰æ—¥æœŸå¾ªç¯ä¸‹è½½ {start_date} åˆ° {end_date} é—´çš„**å…¨å¸‚åœºå†å²æ•°æ®** (å¢é‡ç¼“å­˜)...")

    # 2. å¾ªç¯è·å–å¤æƒå› å­ (adj_factor) å’Œæ—¥çº¿è¡Œæƒ… (daily)
    adj_factor_data_list = []
    daily_data_list = []
    
    download_progress = st.progress(0, text="ä¸‹è½½è¿›åº¦ (æŒ‰æ—¥æœŸå¾ªç¯)...")
    
    for i, date in enumerate(all_dates):
        # æ ¸å¿ƒï¼šè°ƒç”¨ç¼“å­˜å‡½æ•°ï¼Œå¦‚æœå·²ç¼“å­˜åˆ™ç¬é—´è¿”å›
        try:
            cached_data = fetch_and_cache_daily_data(date)
            
            if not cached_data['adj'].empty:
                adj_factor_data_list.append(cached_data['adj'])
                
            if not cached_data['daily'].empty:
                daily_data_list.append(cached_data['daily'])
                
            download_progress.progress((i + 1) / len(all_dates), text=f"ä¸‹è½½è¿›åº¦ï¼šå¤„ç†æ—¥æœŸ {date}")
        
        except Exception as e:
            st.error(f"âŒ è­¦å‘Šï¼šæ—¥æœŸ {date} çš„æ•°æ®æ‹‰å–å¤±è´¥ï¼Œå¯èƒ½æ˜¯ Tushare è¶…æ—¶ã€‚é”™è¯¯ï¼š{e}")
            continue 
    
    download_progress.progress(1.0, text="ä¸‹è½½è¿›åº¦ï¼šåˆå¹¶æ•°æ®...")
    download_progress.empty()

    # 3. åˆå¹¶å’Œå¤„ç†æ•°æ®
    if not adj_factor_data_list:
        st.error("âŒ ä¸¥é‡é”™è¯¯ï¼šæ— æ³•è·å–ä»»ä½•å¤æƒå› å­æ•°æ®ã€‚")
        return False
        
    adj_factor_data = pd.concat(adj_factor_data_list)
    adj_factor_data['adj_factor'] = pd.to_numeric(adj_factor_data['adj_factor'], errors='coerce').fillna(0)
    GLOBAL_ADJ_FACTOR = adj_factor_data.set_index(['ts_code', 'trade_date']).sort_index(level=[0, 1]) 
    
    if not daily_data_list:
        st.error("âŒ ä¸¥é‡é”™è¯¯ï¼šæ— æ³•è·å–ä»»ä½•å†å²æ—¥çº¿æ•°æ®ã€‚")
        return False

    daily_raw_data = pd.concat(daily_data_list)
    GLOBAL_DAILY_RAW = daily_raw_data.set_index(['ts_code', 'trade_date']).sort_index(level=[0, 1])


    # 4. è®¡ç®—å¹¶å­˜å‚¨å…¨å±€å›ºå®š QFQ åŸºå‡†å› å­
    latest_global_date = GLOBAL_ADJ_FACTOR.index.get_level_values('trade_date').max()
    
    if latest_global_date:
        try:
            latest_adj_df = GLOBAL_ADJ_FACTOR.loc[(slice(None), latest_global_date), 'adj_factor']
            GLOBAL_QFQ_BASE_FACTORS = latest_adj_df.droplevel(1).to_dict()
            st.info(f"âœ… å…¨å±€ QFQ åŸºå‡†å› å­å·²è®¾ç½®ã€‚åŸºå‡†æ—¥æœŸ: {latest_global_date}ï¼Œè‚¡ç¥¨æ•°é‡: {len(GLOBAL_QFQ_BASE_FACTORS)}")
        except Exception as e:
            st.error(f"æ— æ³•è®¾ç½®å…¨å±€ QFQ åŸºå‡†å› å­: {e}")
            GLOBAL_QFQ_BASE_FACTORS = {}
    
    
    # 5. è¯Šæ–­ä¿¡æ¯
    st.info(f"âœ… æ•°æ®é¢„åŠ è½½å®Œæˆã€‚æ—¥çº¿æ•°æ®æ€»æ¡ç›®ï¼š{len(GLOBAL_DAILY_RAW)}ï¼Œå¤æƒå› å­æ€»æ¡ç›®ï¼š{len(GLOBAL_ADJ_FACTOR)}")
         
    return True

# ---------------------------
# æ ¸å¿ƒå›æµ‹é€»è¾‘å‡½æ•° (run_backtest_for_a_day)
# ---------------------------
def run_backtest_for_a_day(last_trade, TOP_BACKTEST, FINAL_POOL, MIN_PRICE, MAX_PRICE, MIN_TURNOVER, MIN_AMOUNT, MIN_CIRC_MV_BILLIONS):
    """ä¸ºå•ä¸ªäº¤æ˜“æ—¥è¿è¡Œé€‰è‚¡å’Œå›æµ‹é€»è¾‘"""
    global GLOBAL_DAILY_RAW
    
    # åˆ¤å®šå¸‚åœºçŠ¶æ€ (V30.0 æ ¸å¿ƒ)
    market_state = get_market_state(last_trade)
    
    # æ‹‰å–å…¨å¸‚åœº Daily æ•°æ® 
    daily_all = safe_get('daily', trade_date=last_trade) 
    if daily_all.empty: return pd.DataFrame(), f"æ•°æ®ç¼ºå¤±æˆ–æ‹‰å–å¤±è´¥ï¼š{last_trade}"

    # æ•°æ®åˆå¹¶å’Œåˆæ­¥è¿‡æ»¤...
    df = daily_all.reset_index(drop=True)
    # ... ç»§ç»­æ‰§è¡Œç­›é€‰ä¸è¯„åˆ†

    # å¤„ç†æ•°æ®...
    # è¿›è¡Œç­›é€‰ã€è¯„åˆ†ã€æ”¶ç›Šè®¡ç®—ç­‰æ­¥éª¤

    return fdf.head(TOP_BACKTEST).copy(), None

# ---------------------------
# ä¸»è¿è¡Œå— 
# ---------------------------
if st.button(f"ğŸš€ å¼€å§‹ {BACKTEST_DAYS} æ—¥è‡ªåŠ¨å›æµ‹"):
    
    # è·å–äº¤æ˜“æ—¥
    trade_days_str = get_trade_days(backtest_date_end.strftime("%Y%m%d"), BACKTEST_DAYS)
    if not trade_days_str:
        st.error("æ— æ³•è·å–äº¤æ˜“æ—¥åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥æ—¥æœŸæˆ– Tokenã€‚")
        st.stop()
    
    preload_success = get_all_historical_data(trade_days_str)
    if not preload_success:
        st.error("âŒ å†å²æ•°æ®é¢„åŠ è½½å¤±è´¥ï¼Œå›æµ‹æ— æ³•è¿›è¡Œã€‚è¯·æ£€æŸ¥ Tushare Token å’Œæƒé™ã€‚")
        st.stop()
    
    st.success("âœ… å†å²æ•°æ®é¢„åŠ è½½å®Œæˆï¼QFQ åŸºå‡†å·²å›ºå®šã€‚ç°åœ¨å¼€å§‹æé€Ÿå›æµ‹...")
    
    results_list = []
    total_days = len(trade_days_str)
    
    progress_text = st.empty()
    my_bar = st.progress(0)
    
    for i, trade_date in enumerate(trade_days_str):
        
        progress_text.text(f"â³ æ­£åœ¨å¤„ç†ç¬¬ {i+1}/{total_days} ä¸ªäº¤æ˜“æ—¥ï¼š{trade_date}")
        
        daily_result_df, error = run_backtest_for_a_day(
            trade_date, TOP_BACKTEST, FINAL_POOL, MIN_PRICE, MAX_PRICE, MIN_TURNOVER, MIN_AMOUNT, MIN_CIRC_MV_BILLIONS
        )
        
        if error:
            st.warning(f"è·³è¿‡ {trade_date}ï¼š{error}") 
        elif not daily_result_df.empty:
            daily_result_df['Trade_Date'] = trade_date
            results_list.append(daily_result_df)
            
        my_bar.progress((i + 1) / total_days)

    progress_text.text("âœ… å›æµ‹å®Œæˆï¼Œæ­£åœ¨æ±‡æ€»ç»“æœ...")
    my_bar.empty()
    
    if not results_list:
        st.error("æ‰€æœ‰äº¤æ˜“æ—¥çš„å›æµ‹å‡å¤±è´¥æˆ–æ— ç»“æœã€‚")
        st.stop()
        
    all_results = pd.concat(results_list)
    
    st.header(f"ğŸ“ˆ æœ€ç»ˆå¹³å‡å›æµ‹ç»“æœ (Top {TOP_BACKTEST}ï¼Œå…± {len(all_results['Trade_Date'].unique())} ä¸ªæœ‰æ•ˆäº¤æ˜“æ—¥)")
    # è¾“å‡ºç»“æœ
