# -*- coding: utf-8 -*-
"""
é€‰è‚¡ç‹ Â· 10000 ç§¯åˆ†æ——èˆ°ï¼ˆV5.0S - æ‰¹é‡æ•°æ®è·å– BDF ç¨³å®šç‰ˆï¼‰
è¯´æ˜ï¼š
- **æ ¸å¿ƒä¿®å¤ï¼š** å½»åº•å¼ƒç”¨ per-stock historical data fetchingã€‚æ”¹ä¸ºä¸€æ¬¡æ€§æ‰¹é‡è·å–æ‰€æœ‰å›æµ‹æ—¥æœŸå†…å…¨å¸‚åœºçš„ daily æ•°æ®ã€‚
- **æ•ˆæœï¼š** å½»åº•æ¶ˆé™¤ 18,000 æ¬¡ API è°ƒç”¨ï¼Œå°†æ•°æ®è·å–æ—¶é—´ä» 5-6 å°æ—¶é™ä½åˆ° 5-20 åˆ†é’Ÿã€‚
"""

import streamlit as st
import pandas as pd
import numpy as np
import tushare as ts
from datetime import datetime, timedelta
import warnings
# os/pickle modules are removed as FBC is abandoned

warnings.filterwarnings("ignore")

# ---------------------------
# V5.0S BDF é…ç½®
# ---------------------------
# æ•°æ®åŠ è½½ç¼“å­˜é”®ï¼ˆç”¨äº Streamlit ç¼“å­˜æ‰¹é‡æ•°æ®ï¼‰
BDF_CACHE_KEY = 2.0 

# ---------------------------
# é¡µé¢è®¾ç½® (å…¶ä½™ä»£ç ä¿æŒä¸å˜ï¼Œè¯·ç¡®ä¿å®Œå…¨æ›¿æ¢)
# ---------------------------
st.set_page_config(page_title="é€‰è‚¡ç‹ Â· 10000æ——èˆ°ï¼ˆV5.0S-BDF ç¨³å®šç‰ˆï¼‰", layout="wide")
st.title("é€‰è‚¡ç‹ Â· 10000 ç§¯åˆ†æ——èˆ°ï¼ˆV5.0S - æ‰¹é‡æ•°æ®è·å– BDFï¼‰")
st.markdown("### ğŸš€ ç»ˆæç¨³å®šç‰ˆï¼šæ•°æ®è·å–é€Ÿåº¦æå‡è‡³åˆ†é’Ÿçº§")
st.markdown("è¾“å…¥ä½ çš„ Tushare Tokenï¼ˆä»…æœ¬æ¬¡è¿è¡Œä½¿ç”¨ï¼‰ã€‚")

# ... (ä¾§è¾¹æ å‚æ•°ã€Token è¾“å…¥å’Œè¾…åŠ©å‡½æ•°ä¿æŒä¸å˜ï¼Œè¯·ç¡®ä¿ä½¿ç”¨å®Œæ•´ä»£ç )
# [æ­¤å¤„çœç•¥äº†å¤§éƒ¨åˆ†ä¸ä¸Šä¸€ç‰ˆ FBC ç›¸åŒçš„ä»£ç ï¼Œä½† BDF ç‰ˆæœ¬éœ€è¦å®Œæ•´ä»£ç ]

# ---------------------------
# Token è¾“å…¥ï¼ˆä¸»åŒºï¼‰
# ---------------------------
TS_TOKEN = st.text_input("Tushare Tokenï¼ˆè¾“å…¥åæŒ‰å›è½¦ï¼‰", type="password")
if not TS_TOKEN:
    st.warning("è¯·è¾“å…¥ Tushare Token æ‰èƒ½è¿è¡Œè„šæœ¬ã€‚")
    st.stop()

# åˆå§‹åŒ– tushare
ts.set_token(TS_TOKEN)
pro = ts.pro_api()

# ---------------------------
# å®‰å…¨è°ƒç”¨ & ç¼“å­˜è¾…åŠ© (ä¿æŒä¸å˜)
# ---------------------------
def safe_get(func, **kwargs):
    # ... (ä¸ FBC ç‰ˆæœ¬ç›¸åŒ)

@st.cache_data(ttl=600)
def get_trade_cal(start_date, end_date):
    # ... (ä¸ FBC ç‰ˆæœ¬ç›¸åŒ)

@st.cache_data(ttl=36000) 
def find_last_trade_day(max_days=20):
    # ... (ä¸ FBC ç‰ˆæœ¬ç›¸åŒ)

last_trade = find_last_trade_day()
if not last_trade:
    st.error("æ— æ³•æ‰¾åˆ°æœ€è¿‘äº¤æ˜“æ—¥ï¼Œæ£€æŸ¥ç½‘ç»œæˆ– Token æƒé™ã€‚")
    st.stop()
st.info(f"å‚è€ƒæœ€è¿‘äº¤æ˜“æ—¥ï¼š{last_trade}")

# ---------------------------
# **æ ¸å¿ƒä¿®æ”¹ï¼šæ‰¹é‡æ•°æ®è·å– (BDF)**
# ---------------------------

@st.cache_data(ttl=86400)
def bulk_fetch_daily_data(trade_dates_tuple, bdf_key):
    """
    ä¸€æ¬¡æ€§æ‰¹é‡è·å–æ‰€æœ‰å›æµ‹æ—¥æœŸå†…çš„å…¨å¸‚åœº daily æ•°æ®ã€‚
    ä½¿ç”¨ Streamlit ç¼“å­˜ï¼ˆå› ä¸ºåªæœ‰å‡ åæ¬¡è°ƒç”¨ï¼Œé€Ÿåº¦æå¿«ï¼‰ã€‚
    """
    _ = bdf_key # ç”¨äºæ‰‹åŠ¨åˆ·æ–°æ•°æ®ç¼“å­˜
    data_cache = {}
    st.write(f"æ­£åœ¨æ‰¹é‡è·å– {len(trade_dates_tuple)} ä¸ªäº¤æ˜“æ—¥çš„ daily æ•°æ® (çº¦ {len(trade_dates_tuple)} æ¬¡ API è°ƒç”¨)...")
    pbar = st.progress(0)
    
    for i, date in enumerate(trade_dates_tuple):
        # æ‰¹é‡è·å– Tushare çš„ daily æ•°æ®ï¼ˆå…¨å¸‚åœºï¼‰
        daily_df = safe_get(pro.daily, trade_date=date)
        if not daily_df.empty:
            data_cache[date] = daily_df
        pbar.progress((i + 1) / len(trade_dates_tuple))
    
    pbar.progress(1.0)
    st.success("æ‰¹é‡æ•°æ®åŠ è½½å®Œæˆï¼Œè€—æ—¶åº”åœ¨åˆ†é’Ÿçº§ã€‚")
    return data_cache

# ---------------------------
# **æ ¸å¿ƒä¿®æ”¹ï¼šå†å²æ•°æ®æå– (ä½¿ç”¨ BDF ç¼“å­˜)**
# ---------------------------

# å…¨å±€å˜é‡ï¼Œç”¨äºå­˜å‚¨æ‰¹é‡æ•°æ®
ALL_DAILY_DATA_CACHE = None 

def get_hist_from_bulk(ts_code, end_date, days=60, trade_dates_list=None):
    """
    ä»å…¨å±€çš„ ALL_DAILY_DATA_CACHE ä¸­æå–å•ç¥¨å†å²æ•°æ®ã€‚
    """
    global ALL_DAILY_DATA_CACHE
    
    if ALL_DAILY_DATA_CACHE is None or not trade_dates_list:
        return pd.DataFrame()
    
    # æ‰¾åˆ°æ‰€æœ‰éœ€è¦çš„æ—¥æœŸ
    end_date_index = trade_dates_list.index(end_date)
    start_index = max(0, end_date_index - days * 2) # ç•™è¶³å†—ä½™
    
    required_dates = trade_dates_list[start_index:end_date_index + 1]
    
    history_list = []
    
    for date in required_dates:
        daily_df = ALL_DAILY_DATA_CACHE.get(date)
        if daily_df is not None:
            # åœ¨å…¨å¸‚åœºæ•°æ®ä¸­æŸ¥æ‰¾è¿™åªè‚¡ç¥¨
            stock_data = daily_df[daily_df['ts_code'] == ts_code]
            if not stock_data.empty:
                history_list.append(stock_data.iloc[0])

    if not history_list:
        return pd.DataFrame()
        
    return pd.DataFrame(history_list).sort_values('trade_date').reset_index(drop=True)

# ---------------------------
# é€‰è‚¡é€»è¾‘ (ä½¿ç”¨ BDF)
# ---------------------------
# get_hist_cached å‡½æ•°è¢«ç§»é™¤ï¼Œé€»è¾‘é›†æˆåˆ° compute_scores ä¸­

# ... (compute_indicators, safe_merge_pool, norm_col ä¿æŒä¸å˜)

def compute_scores(trade_date, trade_dates_list):
    """
    è¿è¡Œ T æ—¥çš„é€‰è‚¡ã€æ¸…æ´—å’Œè¯„åˆ†é€»è¾‘ï¼Œè·å–ç»¼åˆè¯„åˆ†ã€‚
    """
    global ALL_DAILY_DATA_CACHE
    
    # ---------------------------
    # 1. æ‹‰å½“æ—¥æ¶¨å¹…æ¦œåˆç­›ï¼ˆä½¿ç”¨ BDF ç¼“å­˜ï¼‰
    # ---------------------------
    daily_all_raw = ALL_DAILY_DATA_CACHE.get(trade_date)
    if daily_all_raw is None or daily_all_raw.empty:
        # å¦‚æœå½“æ—¥æ•°æ®ç¼“å­˜ç¼ºå¤±ï¼Œå°è¯•ç›´æ¥ä» Tushare è·å–ï¼ˆä»…åœ¨å®æ—¶é€‰è‚¡æ—¶æœ‰ç”¨ï¼‰
        daily_all = safe_get(pro.daily, trade_date=trade_date)
    else:
        daily_all = daily_all_raw.copy()
        
    if daily_all.empty:
        return pd.DataFrame()

    daily_all = daily_all.sort_values("pct_chg", ascending=False).reset_index(drop=True)
    pool0 = daily_all.head(int(INITIAL_TOP_N)).copy().reset_index(drop=True)

    # ... (2. åŠ è½½é«˜çº§æ¥å£ åˆ° 3. åŸºæœ¬æ¸…æ´— é€»è¾‘ä¸ FBC ç‰ˆæœ¬ç›¸åŒ)
    
    # ---------------------------
    # 4. è¯„åˆ†æ± é€ç¥¨è®¡ç®—å› å­ (ä½¿ç”¨ BDF æå–å†å²)
    # ---------------------------
    clean_df = clean_df.sort_values("pct_chg", ascending=False).head(FINAL_POOL).copy()
    
    records = []
    for idx, row in enumerate(clean_df.itertuples()):
        ts_code = getattr(row, 'ts_code')
        
        # æ ¸å¿ƒï¼šè°ƒç”¨ BDF ç‰ˆæœ¬çš„å†å²æ•°æ®æå–å‡½æ•°
        hist = get_hist_from_bulk(ts_code, trade_date, days=60, trade_dates_list=trade_dates_list)
        ind = compute_indicators(hist)

        # ... (æŒ‡æ ‡æå–ä¸ FBC ç‰ˆæœ¬ç›¸åŒ)
        
        records.append(rec)
 
    fdf = pd.DataFrame(records)
    
    # ... (5. é£é™©è¿‡æ»¤ åˆ° 7. RSLã€å½’ä¸€åŒ–ä¸è¯„åˆ† é€»è¾‘ä¸ FBC ç‰ˆæœ¬ç›¸åŒ)
    
    return fdf


# ---------------------------
# è¿è¡Œå½“æ—¥é€‰è‚¡
# ---------------------------
if st.button("ğŸš€ è¿è¡Œå½“æ—¥é€‰è‚¡ï¼ˆåˆæ¬¡è¿è¡Œå¯èƒ½è¾ƒä¹…ï¼‰"):
    # å®æ—¶é€‰è‚¡ä¹Ÿéœ€è¦å†å²æ•°æ®ï¼Œé¢„åŠ è½½ 120 å¤©æ—¥å†
    temp_start = (datetime.strptime(last_trade, "%Y%m%d") - timedelta(days=120)).strftime("%Y%m%d")
    temp_trade_dates = get_trade_cal(temp_start, last_trade)
    global ALL_DAILY_DATA_CACHE
    # å®æ—¶é€‰è‚¡ä¾èµ– BDFï¼Œä½†åªåŠ è½½è¿‘æœŸçš„
    ALL_DAILY_DATA_CACHE = bulk_fetch_daily_data(tuple(temp_trade_dates), BDF_CACHE_KEY) 
    
    st.write("æ­£åœ¨æ‹‰å–å½“æ—¥ daily æ•°æ®å¹¶è®¡ç®—è¯„åˆ†...")
    fdf = compute_scores(last_trade, temp_trade_dates)

    # ... (è¯„åˆ†å±•ç¤ºä¸ä¸‹è½½é€»è¾‘ä¸ FBC ç‰ˆæœ¬ç›¸åŒ)

# ---------------------------
# å†å²å›æµ‹éƒ¨åˆ†ï¼ˆBDF ç¨³å®šç‰ˆï¼‰
# ---------------------------
@st.cache_data(ttl=6000)
def run_backtest(start_date, end_date, hold_days, backtest_top_k, bt_cache_key):
    global ALL_DAILY_DATA_CACHE
    _ = bt_cache_key 

    trade_dates = get_trade_cal(start_date, end_date)
    
    if not trade_dates:
        return {h: {'returns': [], 'wins': 0, 'total': 0, 'win_rate': 0.0, 'avg_return': 0.0} for h in hold_days}

    results = {h: {'returns': [], 'wins': 0, 'total': 0, 'win_rate': 0.0, 'avg_return': 0.0} for h in hold_days}
    
    bt_start = (datetime.strptime(end_date, "%Y%m%d") - timedelta(days=BACKTEST_DAYS * 2)).strftime("%Y%m%d")
    buy_dates_pool = [d for d in trade_dates if d >= bt_start and d <= end_date]
    backtest_dates = buy_dates_pool[-BACKTEST_DAYS:]
    
    if len(backtest_dates) < BACKTEST_DAYS:
        st.warning(f"ç”±äºæ•°æ®æˆ–äº¤æ˜“æ—¥é™åˆ¶ï¼Œå›æµ‹ä»…èƒ½è¦†ç›– {len(backtest_dates)} å¤©ã€‚")
    
    # ç¡®å®šå›æµ‹æ‰€éœ€çš„å…¨éƒ¨äº¤æ˜“æ—¥
    required_dates = set(backtest_dates)
    for buy_date in backtest_dates:
        try:
            current_index = trade_dates.index(buy_date)
            for h in hold_days:
                # éœ€è¦ T+1 å’Œ T+1+H çš„ daily æ•°æ®æ¥è®¡ç®—ä¹°å–ä»·
                required_dates.add(trade_dates[current_index + 1]) 
                required_dates.add(trade_dates[current_index + h + 1])
        except (ValueError, IndexError):
            continue
    
    # **æ ¸å¿ƒæ­¥éª¤ï¼šæ‰¹é‡è·å–æ‰€æœ‰å›æµ‹æ—¥æœŸçš„æ•°æ®**
    ALL_DAILY_DATA_CACHE = bulk_fetch_daily_data(tuple(trade_dates), BDF_CACHE_KEY)

    st.write(f"æ­£åœ¨æ¨¡æ‹Ÿ {len(backtest_dates)} ä¸ªäº¤æ˜“æ—¥çš„é€‰è‚¡å›æµ‹...")
    pbar_bt = st.progress(0)
    
    for i, t_day in enumerate(backtest_dates): # T æ—¥ (é€‰è‚¡æ—¥)
        
        # 1. è¿è¡Œ T æ—¥é€‰è‚¡ä¸è¯„åˆ†é€»è¾‘ (ç°åœ¨ get_hist_from_bulk æ˜¯ç¬æ—¶å®Œæˆçš„)
        t_scores = compute_scores(t_day, trade_dates) 
        
        # ... (2. ç¡®å®š T+1 ä¹°å…¥æ—¥ åˆ° 3. ç»“æœè®°å½• é€»è¾‘ä¸ FBC ç‰ˆæœ¬ç›¸åŒ)

        pbar_bt.progress((i+1)/len(backtest_dates))

    pbar_bt.progress(1.0)
    
    # ... (æœ€ç»ˆç»“æœå±•ç¤ºä¸å¯¼å‡ºé€»è¾‘ä¸ FBC ç‰ˆæœ¬ç›¸åŒ)

# ---------------------------
# å›æµ‹æ‰§è¡Œ
# ---------------------------
if st.checkbox("âœ… è¿è¡Œå†å²å›æµ‹", value=False):
    # ... (å›æµ‹æ‰§è¡Œé€»è¾‘ä¸ FBC ç‰ˆæœ¬ç›¸åŒ)
